{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3df7b88f-872f-46b3-bcd2-4a5538efb8e6",
   "metadata": {
    "id": "3df7b88f-872f-46b3-bcd2-4a5538efb8e6"
   },
   "source": [
    "IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48bf358f-7d8a-47a9-b8ae-7dd2ff6a6d85",
   "metadata": {
    "id": "48bf358f-7d8a-47a9-b8ae-7dd2ff6a6d85"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import chess\n",
    "import chess.pgn\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a171567-e256-4646-a73e-1a998a9ff555",
   "metadata": {
    "id": "0a171567-e256-4646-a73e-1a998a9ff555"
   },
   "source": [
    "FEN UTILITY FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff0de1b3-058b-4a97-a711-ba01a7327c02",
   "metadata": {
    "id": "ff0de1b3-058b-4a97-a711-ba01a7327c02"
   },
   "outputs": [],
   "source": [
    "def embed_fen(fen):\n",
    "    board = chess.Board(fen)\n",
    "    embed = [0]*768\n",
    "    piece_dict = {'K': 0, 'Q': 1, 'R': 2, 'B': 3, 'N': 4, 'P': 5, 'k': 6, 'q': 7, 'r': 8, 'b': 9, 'n': 10, 'p': 11}\n",
    "    for square, piece in board.piece_map().items():\n",
    "        embed[square*12 + piece_dict[str(piece)]] = 1\n",
    "    #embed[-1] = int(str(stock_eval(board)))/100 #to include stockfish evaluation\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c633534-e534-4d25-8334-dcd14621f498",
   "metadata": {
    "id": "7c633534-e534-4d25-8334-dcd14621f498"
   },
   "outputs": [],
   "source": [
    "def decode_embed(embed):\n",
    "    piece_dict = {id:piece for id, piece in zip(np.arange(12), ['K', 'Q', 'R', 'B', 'N', 'P', 'k', 'q', 'r', 'b', 'n', 'p'])}\n",
    "    piece_loc = np.arange(len(embed))[embed == 1]\n",
    "    board = chess.Board(None)\n",
    "    for loc in piece_loc:\n",
    "        sq = loc//12\n",
    "        p = piece_dict[loc % 12]\n",
    "        board.set_piece_at(sq, chess.Piece.from_symbol(p))\n",
    "    return board"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9e7f06-bf25-4482-95e0-1a8f44873107",
   "metadata": {
    "id": "4d9e7f06-bf25-4482-95e0-1a8f44873107"
   },
   "source": [
    "POSITIONAL ENCODING ON TOKENIZED INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a527935a-e1ea-423c-9656-68fe64c0b434",
   "metadata": {
    "id": "a527935a-e1ea-423c-9656-68fe64c0b434"
   },
   "outputs": [],
   "source": [
    "def pos_enc_matrix(L, d, n = 10000):\n",
    "    \"\"\"Create positional encoding matrix\n",
    "\n",
    "    Args:\n",
    "        L: Input dimension (length)\n",
    "        d: Output dimension (depth), even only\n",
    "        n: Constant for the sinusoidal functions\n",
    "\n",
    "    Returns:\n",
    "        numpy matrix of floats of dimension L-by-d. At element (k,2i) the value\n",
    "        is sin(k/n^(2i/d)) while at element (k,2i+1) the value is cos(k/n^(2i/d))\n",
    "    \"\"\"\n",
    "    assert d % 2 == 0, \"Output dimension needs to be an even integer\"\n",
    "    d2 = d//2\n",
    "    P = np.zeros((L, d))\n",
    "    k = np.arange(L).reshape(-1, 1)     # L-column vector\n",
    "    i = np.arange(d2).reshape(1, -1)    # d-row vector\n",
    "    denom = np.power(n, -i/d2)          # n**(-2*i/d)\n",
    "    args = k * denom                    # (L,d) matrix\n",
    "    P[:, ::2] = np.sin(args)\n",
    "    P[:, 1::2] = np.cos(args)\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88e36abe-efd9-4862-b597-7de0b9926379",
   "metadata": {
    "id": "88e36abe-efd9-4862-b597-7de0b9926379"
   },
   "outputs": [],
   "source": [
    "#position embedding class for decoder (to be used as a layer)\n",
    "#@tf.keras.utils.register_keras_serializable()\n",
    "class PositionalEmbeddingDecoder(tf.keras.layers.Layer):\n",
    "    \"\"\"Positional embedding layer. Assume tokenized input, transform into\n",
    "    embedding and returns positional-encoded output.\"\"\"\n",
    "    def __init__(self, seq_len, vocab_size, embed_dim, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            seq_len: Input sequence length\n",
    "            vocab_size: Input vocab size, for setting up embedding matrix\n",
    "            embed_dim: Embedding vector size, for setting up embedding matrix\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim     # d_model in paper\n",
    "        # token embedding layer: Convert integer token to D-dim float vector\n",
    "        self.token_embeddings = tf.keras.layers.Embedding(\n",
    "            input_dim = vocab_size, output_dim = embed_dim, mask_zero = True\n",
    "        )\n",
    "        # positional embedding layer: a matrix of hard-coded sine values\n",
    "        matrix = pos_enc_matrix(seq_len, embed_dim)\n",
    "        self.position_embeddings = tf.constant(matrix, dtype = \"float32\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Input tokens convert into embedding vectors then superimposed\n",
    "        with position vectors\"\"\"\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        return embedded_tokens + self.position_embeddings\n",
    "\n",
    "    # this layer is using an Embedding layer, which can take a mask\n",
    "    # passing_mask_tensors_directly_to_layers\n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        return self.token_embeddings.compute_mask(*args, **kwargs)\n",
    "\n",
    "    def get_config(self):\n",
    "        # to make save and load a model using custom layer possible\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"seq_len\": self.seq_len,\n",
    "            \"vocab_size\": self.vocab_size,\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86ec5abd-f339-4685-b1b1-fd5bcbf7131b",
   "metadata": {
    "id": "86ec5abd-f339-4685-b1b1-fd5bcbf7131b"
   },
   "outputs": [],
   "source": [
    "# #position embedding class for encoder (to be used as a layer)\n",
    "# #input is padded (to sequence length) and one hot encoded\n",
    "# #@tf.keras.utils.register_keras_serializable()\n",
    "# class PositionalEmbeddingEncoder(tf.keras.layers.Layer):\n",
    "#     \"\"\"Positional embedding layer. Assume tokenized input, transform into\n",
    "#     embedding and returns positional-encoded output.\"\"\"\n",
    "#     def __init__(self, seq_len, embed_dim, embed_model, **kwargs):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             sequence_length: Input sequence length\n",
    "#             vocab_size: Input vocab size, for setting up embedding matrix\n",
    "#             embed_dim: Embedding vector size, for setting up embedding matrix\n",
    "#         \"\"\"\n",
    "#         super().__init__(**kwargs)\n",
    "#         self.seq_len = seq_len\n",
    "#         self.embed_dim = embed_dim     # d_model in paper\n",
    "#         # token embedding layer: Convert integer token to D-dim float vector\n",
    "#         self.embed_model = embed_model\n",
    "#         #self.token_embeddings = tf.keras.layers.TimeDistributed(embed_model)\n",
    "#         # positional embedding layer: a matrix of hard-coded sine values\n",
    "#         matrix = pos_enc_matrix(seq_len, embed_dim)\n",
    "#         self.position_embeddings = tf.constant(matrix, dtype = \"float32\")\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         \"\"\"Input tokens convert into embedding vectors then superimposed\n",
    "#         with position vectors\"\"\"\n",
    "#         #embedded_tokens = self.token_embeddings(inputs) #keeping the zero rows\n",
    "#         #embedded_tokens = tf.stack([self.embed_model(inputs[i]) for i in range(self.seq_len)])\n",
    "#         embedded_tokens = tf.stack([self.embed_model(inputs[:, i, :]) for i in range(self.seq_len)], axis = 1)\n",
    "#         return embedded_tokens + self.position_embeddings\n",
    "\n",
    "#     # this layer is using an Embedding layer, which can take a mask\n",
    "#     # passing_mask_tensors_directly_to_layers\n",
    "#     def compute_mask(self, inputs, *args, **kwargs):\n",
    "#         return tf.reduce_all(inputs == 0, axis = 2)\n",
    "\n",
    "#     def get_config(self):\n",
    "#         # to make save and load a model using custom layer possible\n",
    "#         config = super().get_config()\n",
    "#         config.update({\n",
    "#             \"seq_len\": self.seq_len,\n",
    "#             \"embed_model\": self.embed_model.to_json(),\n",
    "#             \"embed_dim\": self.embed_dim,\n",
    "#         })\n",
    "#         return config\n",
    "\n",
    "#     @classmethod\n",
    "#     def from_config(cls, config):\n",
    "#         embed_model = tf.keras.models.model_from_json(config.pop('embed_model'))\n",
    "#         return cls(embed_model = embed_model, **config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65d830ad-129a-4b54-a867-fe1005382638",
   "metadata": {},
   "outputs": [],
   "source": [
    "#position embedding class for encoder (to be used as a layer)\n",
    "#input is padded (to sequence length) and one hot encoded\n",
    "#@tf.keras.utils.register_keras_serializable()\n",
    "class PositionalEmbeddingEncoder(tf.keras.layers.Layer):\n",
    "    \"\"\"Positional embedding layer. Assume tokenized input, transform into\n",
    "    embedding and returns positional-encoded output.\"\"\"\n",
    "    def __init__(self, seq_len, embed_dim, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sequence_length: Input sequence length\n",
    "            vocab_size: Input vocab size, for setting up embedding matrix\n",
    "            embed_dim: Embedding vector size, for setting up embedding matrix\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.seq_len = seq_len\n",
    "        self.embed_dim = embed_dim     # d_model in paper\n",
    "        # token embedding layer: Convert integer token to D-dim float vector\n",
    "        # self.embed_model = embed_model\n",
    "        #self.token_embeddings = tf.keras.layers.TimeDistributed(embed_model)\n",
    "        # positional embedding layer: a matrix of hard-coded sine values\n",
    "        matrix = pos_enc_matrix(seq_len, embed_dim)\n",
    "        self.position_embeddings = tf.constant(matrix, dtype = \"float32\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Input tokens convert into embedding vectors then superimposed\n",
    "        with position vectors\"\"\"\n",
    "        #embedded_tokens = self.token_embeddings(inputs) #keeping the zero rows\n",
    "        #embedded_tokens = tf.stack([self.embed_model(inputs[i]) for i in range(self.seq_len)])\n",
    "        #embedded_tokens = tf.stack([self.embed_model(inputs[:, i, :]) for i in range(self.seq_len)], axis = 1)\n",
    "        #return embedded_tokens + self.position_embeddings\n",
    "        return inputs + self.position_embeddings\n",
    "\n",
    "    # this layer is using an Embedding layer, which can take a mask\n",
    "    # passing_mask_tensors_directly_to_layers\n",
    "    def compute_mask(self, inputs, *args, **kwargs):\n",
    "        return tf.reduce_all(inputs == 0, axis = 2)\n",
    "\n",
    "    def get_config(self):\n",
    "        # to make save and load a model using custom layer possible\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"seq_len\": self.seq_len,\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f78e445-b47d-4c80-a28a-002c3a9d57ca",
   "metadata": {},
   "source": [
    "ATTENTION FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6frVtTnmRTSt",
   "metadata": {
    "id": "6frVtTnmRTSt"
   },
   "outputs": [],
   "source": [
    "def self_attention(input_shape, prefix=\"att\", mask=False, **kwargs):\n",
    "    \"\"\"Self-attention layers at transformer encoder and decoder. Assumes its\n",
    "    input is the output from positional encoding layer.\n",
    "\n",
    "    Args:\n",
    "        prefix (str): The prefix added to the layer names\n",
    "        masked (bool): whether to use causal mask. Should be False on encoder and\n",
    "                       True on decoder. When True, a mask will be applied such that\n",
    "                       each location only has access to the locations before it.\n",
    "    \"\"\"\n",
    "    # create layers\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape, dtype='float32',\n",
    "                                   name=f\"{prefix}_in1\")\n",
    "    attention = tf.keras.layers.MultiHeadAttention(name=f\"{prefix}_attn1\", **kwargs)\n",
    "    norm = tf.keras.layers.LayerNormalization(name=f\"{prefix}_norm1\")\n",
    "    add = tf.keras.layers.Add(name=f\"{prefix}_add1\")\n",
    "    # functional API to connect input to output\n",
    "    attout = attention(query=inputs, value=inputs, key=inputs,\n",
    "                       use_causal_mask=mask)\n",
    "    outputs = norm(add([inputs, attout]))\n",
    "    # create model and return\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=f\"{prefix}_att\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ItGNhmF6Shx4",
   "metadata": {
    "id": "ItGNhmF6Shx4"
   },
   "outputs": [],
   "source": [
    "def cross_attention(input_shape, context_shape, prefix=\"att\", **kwargs):\n",
    "    \"\"\"Cross-attention layers at transformer decoder. Assumes its\n",
    "    input is the output from positional encoding layer at decoder\n",
    "    and context is the final output from encoder.\n",
    "\n",
    "    Args:\n",
    "        prefix (str): The prefix added to the layer names\n",
    "    \"\"\"\n",
    "    # create layers\n",
    "    context = tf.keras.layers.Input(shape=context_shape, dtype='float32',\n",
    "                                    name=f\"{prefix}_ctx2\")\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape, dtype='float32',\n",
    "                                   name=f\"{prefix}_in2\")\n",
    "    attention = tf.keras.layers.MultiHeadAttention(name=f\"{prefix}_attn2\", **kwargs)\n",
    "    norm = tf.keras.layers.LayerNormalization(name=f\"{prefix}_norm2\")\n",
    "    add = tf.keras.layers.Add(name=f\"{prefix}_add2\")\n",
    "    # functional API to connect input to output\n",
    "    attout = attention(query=inputs, value=context, key=context)\n",
    "    outputs = norm(add([attout, inputs]))\n",
    "    # create model and return\n",
    "    model = tf.keras.Model(inputs=[(context, inputs)], outputs=outputs,\n",
    "                           name=f\"{prefix}_cross\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "l17Awf5PSoZ1",
   "metadata": {
    "id": "l17Awf5PSoZ1"
   },
   "outputs": [],
   "source": [
    "def feed_forward(input_shape, model_dim, ff_dim, dropout=0.1, prefix=\"ff\"):\n",
    "    \"\"\"Feed-forward layers at transformer encoder and decoder. Assumes its\n",
    "    input is the output from an attention layer with add & norm, the output\n",
    "    is the output of one encoder or decoder block\n",
    "\n",
    "    Args:\n",
    "        model_dim (int): Output dimension of the feed-forward layer, which\n",
    "                         is also the output dimension of the encoder/decoder\n",
    "                         block\n",
    "        ff_dim (int): Internal dimension of the feed-forward layer\n",
    "        dropout (float): Dropout rate\n",
    "        prefix (str): The prefix added to the layer names\n",
    "    \"\"\"\n",
    "    # create layers\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape, dtype='float32',\n",
    "                                   name=f\"{prefix}_in3\")\n",
    "    dense1 = tf.keras.layers.Dense(ff_dim, name=f\"{prefix}_ff1\", activation=\"relu\")\n",
    "    dense2 = tf.keras.layers.Dense(model_dim, name=f\"{prefix}_ff2\")\n",
    "    drop = tf.keras.layers.Dropout(dropout, name=f\"{prefix}_drop\")\n",
    "    add = tf.keras.layers.Add(name=f\"{prefix}_add3\")\n",
    "    # functional API to connect input to output\n",
    "    ffout = drop(dense2(dense1(inputs)))\n",
    "    norm = tf.keras.layers.LayerNormalization(name=f\"{prefix}_norm3\")\n",
    "    outputs = norm(add([inputs, ffout]))\n",
    "    # create model and return\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=f\"{prefix}_ff\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ZlPembyTStg1",
   "metadata": {
    "id": "ZlPembyTStg1"
   },
   "outputs": [],
   "source": [
    "def encoder(input_shape, key_dim, ff_dim, dropout=0.1, prefix=\"enc\", **kwargs):\n",
    "    \"\"\"One encoder unit. The input and output are in the same shape so we can\n",
    "    daisy chain multiple encoder units into one larger encoder\"\"\"\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Input(shape=input_shape, dtype='float32', name=f\"{prefix}_in0\"),\n",
    "        self_attention(input_shape, prefix=prefix, key_dim=key_dim, mask=False, **kwargs),\n",
    "        feed_forward(input_shape, key_dim, ff_dim, dropout, prefix),\n",
    "    ], name=prefix)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "Djop6srISuth",
   "metadata": {
    "id": "Djop6srISuth"
   },
   "outputs": [],
   "source": [
    "def decoder(input_shape, key_dim, ff_dim, dropout=0.1, prefix=\"dec\", **kwargs):\n",
    "    \"\"\"One decoder unit. The input and output are in the same shape so we can\n",
    "    daisy chain multiple decoder units into one larger decoder. The context\n",
    "    vector is also assumed to be the same shape for convenience\"\"\"\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape, dtype='float32',\n",
    "                                   name=f\"{prefix}_in0\")\n",
    "    context = tf.keras.layers.Input(shape=input_shape, dtype='float32',\n",
    "                                    name=f\"{prefix}_ctx0\")\n",
    "    attmodel = self_attention(input_shape, key_dim=key_dim, mask=True,\n",
    "                              prefix=prefix, **kwargs)\n",
    "    crossmodel = cross_attention(input_shape, input_shape, key_dim=key_dim,\n",
    "                                 prefix=prefix, **kwargs)\n",
    "    ffmodel = feed_forward(input_shape, key_dim, ff_dim, dropout, prefix)\n",
    "    x = attmodel(inputs)\n",
    "    x = crossmodel([(context, x)])\n",
    "    output = ffmodel(x)\n",
    "    model = tf.keras.Model(inputs=[(inputs, context)], outputs=output, name=prefix)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b43dcd-8f53-46e7-b60c-3d9d8658d113",
   "metadata": {
    "id": "e4b43dcd-8f53-46e7-b60c-3d9d8658d113"
   },
   "source": [
    "BUILDING THE TRANSFORMER MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "MEQWrHkaTBS1",
   "metadata": {
    "id": "MEQWrHkaTBS1"
   },
   "outputs": [],
   "source": [
    "def transformer(num_layers, num_heads, seq_len_enc, seq_len_dec, key_dim, ff_dim,\n",
    "                vocab_size_tgt, embed_model, dropout=0.1, name=\"transformer\"):\n",
    "    embed_shape_enc = (seq_len_enc, key_dim)  # output shape of the positional embedding layer (encoder)\n",
    "    embed_shape_dec = (seq_len_dec, key_dim)\n",
    "    # set up layers\n",
    "    input_enc = tf.keras.layers.Input(shape=(seq_len_enc, key_dim), dtype=\"float32\",\n",
    "                                      name=\"encoder_inputs\")\n",
    "    input_dec = tf.keras.layers.Input(shape=(seq_len_dec,), dtype=\"int32\",\n",
    "                                      name=\"decoder_inputs\")\n",
    "    #embed_enc = PositionalEmbeddingEncoder(seq_len_enc, vocab_size_src, key_dim, name=\"embed_enc\")\n",
    "    embed_enc = PositionalEmbeddingEncoder(seq_len_enc, key_dim, name = 'embed_enc')\n",
    "    embed_dec = PositionalEmbeddingDecoder(seq_len_dec, vocab_size_tgt, key_dim, name=\"embed_dec\")\n",
    "    encoders = [encoder(input_shape=embed_shape_enc, key_dim=key_dim,\n",
    "                        ff_dim=ff_dim, dropout=dropout, prefix=f\"enc{i}\",\n",
    "                        num_heads=num_heads)\n",
    "                for i in range(num_layers)]\n",
    "    decoders = [decoder(input_shape=embed_shape_dec, key_dim=key_dim,\n",
    "                        ff_dim=ff_dim, dropout=dropout, prefix=f\"dec{i}\",\n",
    "                        num_heads=num_heads)\n",
    "                for i in range(num_layers)]\n",
    "    final = tf.keras.layers.Dense(vocab_size_tgt, name=\"linear\")\n",
    "    # build output\n",
    "    x1 = embed_enc(input_enc)\n",
    "    x2 = embed_dec(input_dec)\n",
    "    for layer in encoders:\n",
    "        x1 = layer(x1)\n",
    "    for layer in decoders:\n",
    "        x2 = layer([x2, x1])\n",
    "    output = final(x2)\n",
    "    # XXX keep this try-except block\n",
    "    try:\n",
    "        del output._keras_mask\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    model = tf.keras.Model(inputs=[input_enc, input_dec], outputs=output, name=name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0e10a7-bd7a-4f61-af30-62e2b7a6ee52",
   "metadata": {
    "id": "cd0e10a7-bd7a-4f61-af30-62e2b7a6ee52"
   },
   "source": [
    "CUSTOM LEARNING RATE SCHEDULING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe6a79d5-36be-494a-8720-f50054599be9",
   "metadata": {
    "id": "fe6a79d5-36be-494a-8720-f50054599be9"
   },
   "outputs": [],
   "source": [
    "#@tf.keras.utils.register_keras_serializable()\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"Custom learning rate for Adam optimizer\"\n",
    "    def __init__(self, key_dim, warmup_steps = 4000):\n",
    "        super().__init__()\n",
    "        self.key_dim = key_dim\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.d = tf.cast(self.key_dim, tf.float32)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, dtype = tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.d) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "    def get_config(self):\n",
    "        # to make save and load a model using custom layer possible0\n",
    "        config = {\n",
    "            \"key_dim\": self.key_dim,\n",
    "            \"warmup_steps\": self.warmup_steps,\n",
    "        }\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20702766-e674-4483-813b-fa3c37ae3325",
   "metadata": {
    "id": "20702766-e674-4483-813b-fa3c37ae3325"
   },
   "source": [
    "LOSS FUNCTION AND EVALUATION METRIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33dd8cf0-1be9-475b-a2f9-3eb2719c4a60",
   "metadata": {
    "id": "33dd8cf0-1be9-475b-a2f9-3eb2719c4a60"
   },
   "outputs": [],
   "source": [
    "def masked_loss(label, pred):\n",
    "    mask = label != 0\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction = 'none')\n",
    "    loss = loss_object(label, pred)\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "    return loss\n",
    "\n",
    "def masked_accuracy(label, pred):\n",
    "    pred = tf.argmax(pred, axis = 2)\n",
    "    label = tf.cast(label, pred.dtype)\n",
    "    match = label == pred\n",
    "    mask = label != 0\n",
    "    match = match & mask\n",
    "    match = tf.cast(match, dtype = tf.float32)\n",
    "    mask = tf.cast(mask, dtype = tf.float32)\n",
    "    return tf.reduce_sum(match)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jK83rGgVRNlZ",
   "metadata": {
    "id": "jK83rGgVRNlZ"
   },
   "source": [
    "READING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38643ca1-1914-4761-aca5-a384cc451ba7",
   "metadata": {
    "id": "38643ca1-1914-4761-aca5-a384cc451ba7"
   },
   "outputs": [],
   "source": [
    "with open('start_fens.txt', 'r') as s:\n",
    "    start_fens = [line.strip() for line in s.readlines()]\n",
    "with open('themes.txt', 'r') as t:\n",
    "    themes = [line.strip() for line in t.readlines()]\n",
    "with open('moves.pkl', 'rb') as m:\n",
    "    moves = pickle.load(m)\n",
    "with open('moves_san.pkl', 'rb') as ms:\n",
    "    moves_san = pickle.load(ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4bcf992-3f17-4875-b993-013cc043d768",
   "metadata": {},
   "outputs": [],
   "source": [
    "moves_san = []\n",
    "for i in range(len(start_fens)):\n",
    "    board = chess.Board(start_fens[i])\n",
    "    moves_san_i = []\n",
    "    for move in moves[i]:\n",
    "        san = board.san(move)\n",
    "        san = san[:-1] if san[-1] in ['+', '#'] else san\n",
    "        moves_san_i.append(san)\n",
    "        board.push(move)\n",
    "    moves_san.append(moves_san_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a385687b-6867-450f-b605-7c465eeba0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('moves_san_nosym.pkl', 'wb') as ms:\n",
    "    pickle.dump(moves_san, ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1r9Idfg2RfpX",
   "metadata": {
    "id": "1r9Idfg2RfpX"
   },
   "outputs": [],
   "source": [
    "indices = [i for i in range(len(start_fens)) if len(themes[i].split(' ')) <= 8 and len(moves[i]) < 9] #filtering the data based on analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e588ab3-ba06-479b-8ec1-3a91c9e7881c",
   "metadata": {
    "id": "2e588ab3-ba06-479b-8ec1-3a91c9e7881c"
   },
   "source": [
    "TOKENIZING THE DECODER I/O (ONLY ON TRAINING DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8d96ee9-b58a-45b1-bd8f-d13105db9352",
   "metadata": {
    "id": "f8d96ee9-b58a-45b1-bd8f-d13105db9352"
   },
   "outputs": [],
   "source": [
    "#Note: vocab_size and seq_length are to be determined after careful examination of the dataset (done above)!\n",
    "vocab_size = 37 #tbd from data (themes)\n",
    "seq_len_dec = 8 #tbd from data\n",
    "#seq_len_enc = 9 #tbd from data\n",
    "seq_len_enc = 8\n",
    "vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens = vocab_size,\n",
    "    standardize = None,\n",
    "    split = \"whitespace\",\n",
    "    output_mode = \"int\",\n",
    "    output_sequence_length = seq_len_dec + 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0e7deb-4e32-4ae1-8cdd-10d0816b3cc2",
   "metadata": {
    "id": "7b0e7deb-4e32-4ae1-8cdd-10d0816b3cc2"
   },
   "source": [
    "TRAIN-TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b06a39ee-e5a5-46da-9200-42fcc80c83e2",
   "metadata": {
    "id": "b06a39ee-e5a5-46da-9200-42fcc80c83e2"
   },
   "outputs": [],
   "source": [
    "ind_train, ind_test = train_test_split(indices, test_size = 0.25, shuffle = True, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7fa77af-2c1e-4db5-bd4f-100ad514122b",
   "metadata": {
    "id": "a7fa77af-2c1e-4db5-bd4f-100ad514122b"
   },
   "outputs": [],
   "source": [
    "themes_train = [themes[i] for i in ind_train]\n",
    "vectorizer.adapt(themes_train) #fitting the vectorizer on the training themes data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6480ccc2-5ead-4528-a306-e3a0a998e0f3",
   "metadata": {
    "id": "6480ccc2-5ead-4528-a306-e3a0a998e0f3"
   },
   "source": [
    "PREPARING THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15388e04-5eba-4bb8-a9f3-9d52e507fa0d",
   "metadata": {
    "id": "15388e04-5eba-4bb8-a9f3-9d52e507fa0d"
   },
   "outputs": [],
   "source": [
    "# # train and validation data generator\n",
    "# #def data_generator(X_enc, X_dec, indices, seq_len, key_dim, vectorizer, batch_size = 32):\n",
    "# def data_generator(start_fens, moves, themes, indices, seq_len_enc, vectorizer, batch_size = 32):\n",
    "#     # vectorizer is the tokenization object for the decoder input/output\n",
    "#     z = [0]*768\n",
    "#     while True:\n",
    "#         for start in range(0, len(indices), batch_size):\n",
    "#             batch_indices = indices[start: start + batch_size]\n",
    "#             batch_X_enc = []\n",
    "#             batch_X_dec = []\n",
    "#             batch_Y = []\n",
    "#             for i in batch_indices:\n",
    "#                 # shape(x) = (batch_size, seq_len, embed_dim)\n",
    "#                 fens = [start_fens[i]]\n",
    "#                 board = chess.Board(start_fens[i])\n",
    "#                 for move in moves[i]:\n",
    "#                     board.push(move)\n",
    "#                     fens.append(board.fen())\n",
    "#                 batch_X_enc_i = [embed_fen(fen) for fen in fens]\n",
    "#                 batch_X_enc_i += [z]*(seq_len_enc - len(fens)) #encoder input\n",
    "#                 batch_X_enc.append(batch_X_enc_i)\n",
    "#                 vect_i = vectorizer(themes[i]).numpy()\n",
    "#                 batch_X_dec.append(vect_i[:-1]) #decoder input\n",
    "#                 batch_Y.append(vect_i[1:]) #decoder target (including the end sentinel shifted right by 1)\n",
    "#             yield [np.array(batch_X_enc), np.array(batch_X_dec)], np.array(batch_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "091227cc-b76d-4d69-9244-c56688ea69df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and validation data generator\n",
    "#def data_generator(X_enc, X_dec, indices, seq_len, key_dim, vectorizer, batch_size = 32):\n",
    "def data_generator(moves_san, themes, indices, seq_len_enc, key_dim, embed_model, vectorizer, batch_size = 32):\n",
    "    # vectorizer is the tokenization object for the decoder input/output\n",
    "    z = [0]*key_dim\n",
    "    while True:\n",
    "        for start in range(0, len(indices), batch_size):\n",
    "            batch_indices = indices[start: start + batch_size]\n",
    "            batch_X_enc = []\n",
    "            batch_X_dec = []\n",
    "            batch_Y = []\n",
    "            for i in batch_indices:\n",
    "                # shape(x) = (batch_size, seq_len, embed_dim)\n",
    "                batch_X_enc_i = []\n",
    "                for move in moves_san[i]:\n",
    "                    if move[-1] == '#':\n",
    "                        move = move[:-1]\n",
    "                    try:\n",
    "                        batch_X_enc_i.append(embed_model.wv[move])\n",
    "                    except:\n",
    "                        break\n",
    "                #batch_X_enc_i = [embed_model.wv[move] for move in moves_san[i]]\n",
    "                batch_X_enc_i += [z]*(seq_len_enc - len(batch_X_enc_i)) #encoder input\n",
    "                batch_X_enc.append(batch_X_enc_i)\n",
    "                vect_i = vectorizer(themes[i]).numpy()\n",
    "                batch_X_dec.append(vect_i[:-1]) #decoder input\n",
    "                batch_Y.append(vect_i[1:]) #decoder target (including the end sentinel shifted right by 1)\n",
    "            yield [np.array(batch_X_enc), np.array(batch_X_dec)], np.array(batch_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f388fed-3d20-4b7f-a151-8bff10cb1389",
   "metadata": {
    "id": "0f388fed-3d20-4b7f-a151-8bff10cb1389"
   },
   "source": [
    "TRAINING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "l9vLkZNAO1iW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l9vLkZNAO1iW",
    "outputId": "cd437a00-db97-40db-b07c-391517e355fc"
   },
   "outputs": [],
   "source": [
    "#embed_model = tf.keras.models.load_model('fen_embed.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "920e5f1d-af6d-4dd5-88d5-7237d1d4e01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = Word2Vec.load('move2vec2.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1e4941a9-3c4f-46b5-82ee-1ad2db7831dd",
   "metadata": {
    "id": "1e4941a9-3c4f-46b5-82ee-1ad2db7831dd"
   },
   "outputs": [],
   "source": [
    "#comb tried = [4,8,192,512,0.1]\n",
    "num_layers = 4\n",
    "num_heads = 8\n",
    "key_dim = 20\n",
    "ff_dim = 64\n",
    "dropout = 0.1\n",
    "model = transformer(num_layers, num_heads, seq_len_enc, seq_len_dec, key_dim, ff_dim, vocab_size, embed_model, dropout)\n",
    "lr = CustomSchedule(key_dim)\n",
    "optimizer = tf.keras.optimizers.Adam(lr, beta_1 = 0.9, beta_2 = 0.98, epsilon = 1e-9)\n",
    "model.compile(loss = masked_loss, optimizer = optimizer, metrics = [masked_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qcwK0qVruw99",
   "metadata": {
    "id": "qcwK0qVruw99"
   },
   "source": [
    "SAMPLE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "GbJ_WElxgN0I",
   "metadata": {
    "id": "GbJ_WElxgN0I"
   },
   "outputs": [],
   "source": [
    "# v = vectorizer(themes_train[:1000]).numpy()\n",
    "# Y = v[:, 1:]\n",
    "# X_dec = v[:, :-1]\n",
    "# X_enc = []\n",
    "# z = [0]*768\n",
    "# for i in ind_train[:1000]:\n",
    "#   fens = [start_fens[i]]\n",
    "#   board = chess.Board(start_fens[i])\n",
    "#   for move in moves[i]:\n",
    "#       board.push(move)\n",
    "#       fens.append(board.fen())\n",
    "#   batch_i_enc = [embed_fen(fen) for fen in fens]\n",
    "#   batch_i_enc += [z]*(seq_len_enc - len(fens))\n",
    "#   print(np.array(batch_i_enc).shape, len(fens))\n",
    "#   X_enc.append(batch_i_enc)\n",
    "# X = [np.array(X_enc), X_dec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6aadc1a3-19e4-4d47-8ab3-1b32052b16b3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6aadc1a3-19e4-4d47-8ab3-1b32052b16b3",
    "outputId": "50126e2c-3590-43f7-bb2f-791b9869328b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562/1562 [==============================] - 934s 584ms/step - loss: 1.8950 - masked_accuracy: 0.4951\n"
     ]
    }
   ],
   "source": [
    "#input = [encoder input, decoder input]\n",
    "#target = [decoder output]\n",
    "batch_size = 64\n",
    "epochs = 1\n",
    "steps_per_epoch = len(ind_train[:100000]) // batch_size\n",
    "#steps_per_epoch = 1000 // batch_size\n",
    "train_data_generator = data_generator(moves_san, themes, ind_train[:100000], seq_len_enc, key_dim, embed_model, vectorizer, batch_size)\n",
    "model.fit(train_data_generator, steps_per_epoch = steps_per_epoch, epochs = epochs)\n",
    "#model.fit(X, Y, epochs = 1, batch_size = 32)\n",
    "train_data_generator.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HX79ejvYSBnF",
   "metadata": {
    "id": "HX79ejvYSBnF"
   },
   "outputs": [],
   "source": [
    "model.save('trans_model2.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pF3SPvsXCjto",
   "metadata": {
    "id": "pF3SPvsXCjto"
   },
   "source": [
    "LOADING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IdpRsChZhTru",
   "metadata": {
    "id": "IdpRsChZhTru"
   },
   "outputs": [],
   "source": [
    "custom_objects = {\"PositionalEmbeddingDecoder\": PositionalEmbeddingDecoder,\n",
    "                  \"PositionalEmbeddingEncoder\": PositionalEmbeddingEncoder,\n",
    "                  \"CustomSchedule\": CustomSchedule,\n",
    "                  \"masked_loss\": masked_loss,\n",
    "                  \"masked_accuracy\": masked_accuracy}\n",
    "with tf.keras.utils.custom_object_scope(custom_objects):\n",
    "    model_loaded = tf.keras.models.load_model(\"trans_model2.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6jSZrrY7GztC",
   "metadata": {
    "id": "6jSZrrY7GztC"
   },
   "source": [
    "MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "Spv0eNJ0G3SN",
   "metadata": {
    "id": "Spv0eNJ0G3SN"
   },
   "outputs": [],
   "source": [
    "# def tagging(start_fen, moves, seq_len_enc, vectorizer, transformer_model):\n",
    "#   z = [0] * 768\n",
    "#   lookup = list(vectorizer.get_vocabulary())\n",
    "#   start_sentinel, end_sentinel = '[start]', '[end]'\n",
    "#   output = [start_sentinel]\n",
    "#   for i in range(seq_len_enc):\n",
    "#     fens = [start_fen]\n",
    "#     board = chess.Board(start_fen)\n",
    "#     for move in moves:\n",
    "#         board.push(move)\n",
    "#         fens.append(board.fen())\n",
    "#     X_enc = [embed_fen(fen) for fen in fens]\n",
    "#     X_enc += [z] * (seq_len_enc - len(fens))\n",
    "#     vector = vectorizer(' '.join(output)).numpy()\n",
    "#     X_dec = vector[:-1]\n",
    "#     pred = transformer_model.predict([np.array([X_enc]), np.array([X_dec])])\n",
    "#     tag = lookup[np.argmax(pred[0, i, :])]\n",
    "#     print(tag)\n",
    "#     output.append(tag)\n",
    "#     if tag == end_sentinel:\n",
    "#       break\n",
    "#   return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b7204569-56ff-439e-bdfd-1e48dee0ce9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagging(moves_san, seq_len_enc, key_dim, vectorizer, embed_model, transformer_model):\n",
    "    z = [0] * key_dim\n",
    "    lookup = list(vectorizer.get_vocabulary())\n",
    "    start_sentinel, end_sentinel = '[start]', '[end]'\n",
    "    output = [start_sentinel]\n",
    "    X_enc = []\n",
    "    for move in moves_san:\n",
    "            if move[-1] == '#':\n",
    "                move = move[:-1]\n",
    "            try:\n",
    "                X_enc.append(embed_model.mv[move])\n",
    "            except:\n",
    "                break\n",
    "    #X_enc = [embed_fen(fen) for fen in fens]\n",
    "    X_enc += [z] * (seq_len_enc - len(X_enc))\n",
    "    for i in range(seq_len_enc):\n",
    "        vector = vectorizer(' '.join(output)).numpy()\n",
    "        X_dec = vector[:-1]\n",
    "        pred = transformer_model.predict([np.array([X_enc]), np.array([X_dec])])\n",
    "        tag = lookup[np.argmax(pred[0, i, :])]\n",
    "        print(tag)\n",
    "        output.append(tag)\n",
    "        if tag == end_sentinel:\n",
    "          break\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7e67c565-386e-46cc-a8ba-e5df78e7dba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[start] defensiveMove [end]'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "themes[ind_test[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "r7iPStZhMYVE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r7iPStZhMYVE",
    "outputId": "53dd6652-01b7-49b9-b93d-8c1c3f35cee9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 235ms/step\n",
      "forcedMate\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "[end]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[start]', 'forcedMate', '[end]']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tagging(start_fens[ind_test[100]], moves[ind_test[10]], seq_len_enc, vectorizer, model)\n",
    "tagging(moves_san[ind_test[10]], seq_len_enc, key_dim, vectorizer, embed_model, model)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
