{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53f23a17-9200-4687-bf14-a8a724cbd60e",
   "metadata": {
    "id": "53f23a17-9200-4687-bf14-a8a724cbd60e"
   },
   "source": [
    "IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0355423-3563-4ddd-9142-e710a1b4fa82",
   "metadata": {
    "id": "f0355423-3563-4ddd-9142-e710a1b4fa82",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import chess\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9f6299-0161-440b-968c-f95b1ce4cd0a",
   "metadata": {
    "id": "4a9f6299-0161-440b-968c-f95b1ce4cd0a"
   },
   "source": [
    "UTILITY FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f34b306-6e54-448a-bca0-f80295b07a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beep():\n",
    "    beep_sound = Audio(filename = \"beep-01.wav\", autoplay = True)\n",
    "    return beep_sound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c8e9cd-dbab-4c3f-8c34-84172da3a1f3",
   "metadata": {
    "id": "a9c8e9cd-dbab-4c3f-8c34-84172da3a1f3"
   },
   "source": [
    "ENCODING THE BOARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab7f3f6e-6d97-4a88-808a-467132ec682c",
   "metadata": {
    "id": "ab7f3f6e-6d97-4a88-808a-467132ec682c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_fen(fen):\n",
    "    file_to_num = {'a': 1, 'b': 2, 'c': 3, 'd':4, 'e':5, 'f':6, 'g':7, 'h':8}\n",
    "    enc = np.zeros([8,8,18]).astype(int)\n",
    "    fen_elem = fen.split(' ')\n",
    "    fen_pos = fen_elem[0]\n",
    "    if fen_elem[1] == 'w':\n",
    "        player = 1\n",
    "        enc_dict = {\"R\":0, \"N\":1, \"B\":2, \"Q\":3, \"K\":4, \"P\":5, \"r\":6, \"n\":7, \"b\":8, \"q\":9, \"k\":10, \"p\":11}\n",
    "    else:\n",
    "        player = 0\n",
    "        enc_dict = {\"r\":0, \"n\":1, \"b\":2, \"q\":3, \"k\":4, \"p\":5, \"R\":6, \"N\":7, \"B\":8, \"Q\":9, \"K\":10, \"P\":11}\n",
    "    enc[:,:,12] = player\n",
    "    castle = fen_elem[2]\n",
    "    if player:\n",
    "        if 'Q' not in castle:\n",
    "            enc[:,:,13] = 1\n",
    "        if 'K' not in castle:\n",
    "            enc[:,:,14] = 1\n",
    "        if 'q' not in castle:\n",
    "            enc[:,:,15] = 1\n",
    "        if 'k' not in castle:\n",
    "            enc[:,:,16] = 1\n",
    "    else:\n",
    "        if 'Q' not in castle:\n",
    "            enc[:,:,15] = 1\n",
    "        if 'K' not in castle:\n",
    "            enc[:,:,16] = 1\n",
    "        if 'q' not in castle:\n",
    "            enc[:,:,13] = 1\n",
    "        if 'k' not in castle:\n",
    "            enc[:,:,14] = 1\n",
    "    enc[:,:,17] = int(fen_elem[-1])\n",
    "    ranks = fen_pos.split('/')\n",
    "    for i, rank in enumerate(ranks):\n",
    "        j = 0\n",
    "        k = 0\n",
    "        while k < len(rank):\n",
    "            if rank[k].isdigit():\n",
    "                j += int(rank[k])\n",
    "                k += 1\n",
    "                continue\n",
    "            enc[(7 - i)*player + (1 - player)*i, j*player + (1 - player)*(7 - j), enc_dict[rank[k]]] = 1\n",
    "            j += 1\n",
    "            k += 1\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5debe940-2046-4e30-b4e2-cfd39fe83d3b",
   "metadata": {
    "id": "5debe940-2046-4e30-b4e2-cfd39fe83d3b"
   },
   "source": [
    "ENCODING THE ACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed7b5c91-73cc-451b-9037-129f46b0d791",
   "metadata": {
    "id": "ed7b5c91-73cc-451b-9037-129f46b0d791",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_actions(move, fen):\n",
    "    if fen.split(' ')[1] == 'w':\n",
    "        player = 1\n",
    "    else:\n",
    "        player = 0\n",
    "    enc = np.zeros([8,8,73]).astype(int)\n",
    "    file_to_num = {'a': 1, 'b': 2, 'c': 3, 'd':4, 'e':5, 'f':6, 'g':7, 'h':8}\n",
    "    init_file = file_to_num[move[0]]\n",
    "    init_rank = int(move[1])\n",
    "    final_file = file_to_num[move[2]]\n",
    "    final_rank = int(move[3])\n",
    "    under_promo = move[-1] if move[-1] in ['n', 'N', 'b', 'B', 'r', 'R'] else None\n",
    "    file_diff = (init_file - final_file)*player + (final_file - init_file)*(1 - player)\n",
    "    rank_diff = (init_rank - final_rank)*player + (final_rank - init_rank)*(1 - player)\n",
    "    if under_promo is not None: #underpromotions\n",
    "        if file_diff == 0: #no capture\n",
    "            if under_promo in ['r', 'R']:\n",
    "                idx = 64\n",
    "            elif under_promo in ['b', 'B']:\n",
    "                idx = 65\n",
    "            else:\n",
    "                idx = 66\n",
    "        elif file_diff > 0: # left capture\n",
    "            if under_promo in ['r', 'R']:\n",
    "                idx = 67\n",
    "            elif under_promo in ['b', 'B']:\n",
    "                idx = 68\n",
    "            else:\n",
    "                idx = 69\n",
    "        else: # right capture\n",
    "            if under_promo in ['r', 'R']:\n",
    "                idx = 70\n",
    "            elif under_promo in ['b', 'B']:\n",
    "                idx = 71\n",
    "            else:\n",
    "                idx = 72\n",
    "    elif file_diff == 0: #NS direction\n",
    "        if rank_diff < 0: #upward\n",
    "            idx = -rank_diff - 1\n",
    "        else: #downward\n",
    "            idx = 6 + rank_diff\n",
    "    elif rank_diff == 0: #EW direction\n",
    "        if file_diff < 0: #right\n",
    "            idx = 13 - file_diff\n",
    "        else: #left\n",
    "            idx = 20 + file_diff\n",
    "    elif abs(file_diff) == abs(rank_diff): #diagonal moves\n",
    "        if rank_diff < 0 and file_diff < 0: #NE\n",
    "            idx = 27 - rank_diff\n",
    "        elif rank_diff > 0 and file_diff > 0: #SW\n",
    "            idx = 34 + rank_diff\n",
    "        elif rank_diff < 0 and file_diff > 0: #NW\n",
    "            idx = 41 + file_diff\n",
    "        else: #SE\n",
    "            idx = 48 + rank_diff\n",
    "    elif file_diff == 1 and rank_diff == -2:\n",
    "        idx = 56\n",
    "    elif file_diff == -1 and rank_diff == -2:\n",
    "        idx = 57\n",
    "    elif file_diff == 2 and rank_diff == 1:\n",
    "        idx = 58\n",
    "    elif file_diff == 2 and rank_diff == -1:\n",
    "        idx = 59\n",
    "    elif file_diff == 1 and rank_diff == 2:\n",
    "        idx = 60\n",
    "    elif file_diff == -1 and rank_diff == 2:\n",
    "        idx = 61\n",
    "    elif file_diff == -2 and rank_diff == 1:\n",
    "        idx = 62\n",
    "    elif file_diff == -2 and rank_diff == -1:\n",
    "        idx = 63\n",
    "    enc[(init_rank - 1)*player + (8 - init_rank)*(1 - player), (init_file - 1)*player + (8 - init_file)*(1 - player), idx] = 1\n",
    "    return enc.flatten(), (init_file - 1)*player + (8 - init_file)*(1 - player), (init_rank - 1)*player + (8 - init_rank)*(1 - player), idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a781a7c4-1a62-425b-afad-f7b29aa54f0b",
   "metadata": {
    "id": "a781a7c4-1a62-425b-afad-f7b29aa54f0b"
   },
   "source": [
    "DECODING THE TENSOR OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a475797-ec97-4f2d-871e-208d55c2dd78",
   "metadata": {
    "id": "7a475797-ec97-4f2d-871e-208d55c2dd78",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decode_action(out_policy, fen):\n",
    "    board = chess.Board(fen)\n",
    "    legal_moves = list(board.legal_moves)\n",
    "    legal_moves_str = np.array([str(move) for move in legal_moves])\n",
    "    prob = np.zeros(len(legal_moves))\n",
    "    for i, move in enumerate(legal_moves_str):\n",
    "        _, x, y, z = parse_actions(move, fen) #x = file, y = rank\n",
    "        prob[i] = out_policy[y, x, z]\n",
    "    prob /= np.sum(prob)\n",
    "    idx = prob.argsort()[::-1]\n",
    "    prob = prob[idx]\n",
    "    legal_moves_str = legal_moves_str[idx]\n",
    "    policy_dict = {move: p for move, p in zip(legal_moves_str, prob)}\n",
    "    best_move = legal_moves_str[0]\n",
    "    return policy_dict, best_move"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8113478e-2107-4530-9e70-1f1d562f623b",
   "metadata": {
    "id": "8113478e-2107-4530-9e70-1f1d562f623b"
   },
   "source": [
    "COLLECTING DATA FROM GRANDMASTER GAMES FOR TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "994eb3bb-91d1-40b8-9a85-4aa4f8428b05",
   "metadata": {
    "id": "994eb3bb-91d1-40b8-9a85-4aa4f8428b05"
   },
   "outputs": [],
   "source": [
    "# fen = []\n",
    "# action = []\n",
    "# all_fen = []\n",
    "# pre_sum = []\n",
    "# game_nos = []\n",
    "with open('move.txt', 'r') as fm:\n",
    "    action = [line.strip() for line in fm.readlines()]\n",
    "with open('all_fen.txt', 'r') as faf:\n",
    "    all_fen = [line.strip() for line in faf.readlines()]\n",
    "with open('fen.txt', 'r') as ff:\n",
    "    fen = [line.strip() for line in ff.readlines()]\n",
    "with open('value_fen.txt', 'r') as vf:\n",
    "    value_fen = [line.strip() for line in vf.readlines()]\n",
    "with open('values.txt', 'r') as v:\n",
    "    values = [int(line.strip()) for line in v.readlines()]\n",
    "with open('game_nos.txt', 'r') as fg:\n",
    "    game_nos = [int(line.strip()) for line in fg.readlines()]\n",
    "with open('pre_sum.txt', 'r') as ps:\n",
    "    pre_sum = [int(line.strip()) for line in ps.readlines()]\n",
    "# with open('puzzle_fen.txt', 'r') as fpf:\n",
    "#     fen += [line.strip() for line in fpf.readlines()]\n",
    "# with open('puzzle_action.txt', 'r') as fpa:\n",
    "#     action += [line.strip() for line in fpa.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d86ed40-a2c5-4d5d-89b7-c15c1af99ad3",
   "metadata": {
    "id": "7d86ed40-a2c5-4d5d-89b7-c15c1af99ad3"
   },
   "source": [
    "TRAIN-TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d06d98c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_train = np.arange(len(fen))\n",
    "value_ind_train = np.arange(len(value_fen))\n",
    "random.seed(0)\n",
    "random.shuffle(ind_train)\n",
    "random.seed(42)\n",
    "random.shuffle(value_ind_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca40980d",
   "metadata": {},
   "source": [
    "DATA GENERATOR WITH HISTORY INCLUDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf0d833d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_data_generator(X, Y, indices, game_nos, pre_sum, all_fen, batch_size, is_validation = False, history = 3):\n",
    "    # Here X and Y are the original data (before splitting)\n",
    "    # total stack height = history + 1\n",
    "    while True:\n",
    "        for start in range(0, len(X), batch_size):\n",
    "            batch_indices = indices[start: start + batch_size]\n",
    "            if history > 0:\n",
    "                batch_X = []\n",
    "                for i in batch_indices:\n",
    "                    board = chess.Board(X[i])\n",
    "                    n_moves = board.fullmove_number\n",
    "                    hash_id = pre_sum[game_nos[i]] + n_moves - 1\n",
    "                    stack_height = 18*(history + 1)\n",
    "                    stack_input = np.empty((8, 8, stack_height))\n",
    "                    for j in range(history, -1, -1):\n",
    "                        stack_input[:, :, 17*(history - j):17*(history + 1 - j)] = parse_fen(all_fen[hash_id - j])\n",
    "                    batch_X.append(stack_input)\n",
    "            else:\n",
    "                batch_X = [parse_fen(X[i]) for i in batch_indices]               \n",
    "            batch_Y = [parse_actions(Y[i], X[i])[0] for i in batch_indices]\n",
    "            yield np.array(batch_X), np.array(batch_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6a60d5d-bd48-4d4a-8c6b-33bf7b43a542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_data_generator(X, Y, indices, batch_size):\n",
    "    # Here X and Y are the original data (before splitting)\n",
    "    # total stack height = history + 1\n",
    "    while True:\n",
    "        for start in range(0, len(X), batch_size):\n",
    "            batch_indices = indices[start: start + batch_size]\n",
    "            batch_X = [parse_fen(X[i]) for i in batch_indices] \n",
    "            batch_Y = [Y[i] for i in batch_indices]\n",
    "            yield np.array(batch_X), np.array(batch_Y).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337efbd4-e97c-45b6-800a-2aea089932a8",
   "metadata": {
    "id": "337efbd4-e97c-45b6-800a-2aea089932a8"
   },
   "source": [
    "POLICY NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "930f61fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable()\n",
    "class ConvBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, kernel_size, **kwargs):\n",
    "        super(ConvBlock, self).__init__(**kwargs)\n",
    "        self.conv = tf.keras.layers.Conv2D(filters, kernel_size, padding = 'same')\n",
    "        self.batch_norm = tf.keras.layers.BatchNormalization()\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class ResBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, kernel_size, **kwargs):\n",
    "        super(ResBlock, self).__init__(**kwargs)\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters, kernel_size, padding = 'same')\n",
    "        self.batch_norm1 = tf.keras.layers.BatchNormalization()\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters, kernel_size, padding = 'same')\n",
    "        self.batch_norm2 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        out = self.conv1(inputs)\n",
    "        out = self.batch_norm1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.batch_norm2(out)\n",
    "        return self.relu(out + inputs)\n",
    "    \n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class PolicyHead(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, kernel_size, num_move_planes, **kwargs):\n",
    "        super(PolicyHead, self).__init__(**kwargs)\n",
    "        self.conv = tf.keras.layers.Conv2D(filters, kernel_size, padding = 'same')\n",
    "        self.batch_norm = tf.keras.layers.BatchNormalization()\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense = tf.keras.layers.Dense(num_move_planes * 8 * 8, activation = 'softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class ValueHead(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, kernel_size, **kwargs):\n",
    "        super(ValueHead, self).__init__(**kwargs)\n",
    "        self.conv = tf.keras.layers.Conv2D(filters, kernel_size, padding = 'same')\n",
    "        self.batch_norm = tf.keras.layers.BatchNormalization()\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(64, activation = 'relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(1, activation = 'tanh')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a2ee749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the policy model\n",
    "history = 0\n",
    "policy_model = tf.keras.Sequential([\n",
    "    ConvBlock(filters = 256, kernel_size = (3, 3), input_shape = (8, 8, 18*(history + 1))),\n",
    "    ResBlock(filters = 256, kernel_size = (3, 3)),\n",
    "    ResBlock(filters = 256, kernel_size = (3, 3)),\n",
    "    ResBlock(filters = 256, kernel_size = (3, 3)),\n",
    "    ResBlock(filters = 256, kernel_size = (3, 3)),\n",
    "    ResBlock(filters = 256, kernel_size = (3, 3)),\n",
    "    ResBlock(filters = 256, kernel_size = (3, 3)),\n",
    "    ResBlock(filters = 256, kernel_size = (3, 3)),\n",
    "    ResBlock(filters = 256, kernel_size = (3, 3)),\n",
    "    ResBlock(filters = 256, kernel_size = (3, 3)),\n",
    "    ResBlock(filters = 256, kernel_size = (3, 3)),\n",
    "    PolicyHead(filters = 128, kernel_size = (1, 1), num_move_planes = 73)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "policy_model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d43f959-7817-4761-b437-85488e49d994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the value model\n",
    "history = 0\n",
    "value_model = tf.keras.Sequential([\n",
    "    ConvBlock(filters = 256, kernel_size = (3, 3), input_shape = (8, 8, 18*(history + 1))),\n",
    "    ResBlock(filters = 256, kernel_size = (3, 3)),\n",
    "    ResBlock(filters = 256, kernel_size = (3, 3)),\n",
    "    ResBlock(filters = 256, kernel_size = (3, 3)),\n",
    "    ResBlock(filters = 256, kernel_size = (3, 3)),\n",
    "    ResBlock(filters = 256, kernel_size = (3, 3)),\n",
    "    ResBlock(filters = 256, kernel_size = (3, 3)),\n",
    "    ResBlock(filters = 256, kernel_size = (3, 3)),\n",
    "    ResBlock(filters = 256, kernel_size = (3, 3)),\n",
    "    ResBlock(filters = 256, kernel_size = (3, 3)),\n",
    "    ResBlock(filters = 256, kernel_size = (3, 3)),\n",
    "    ValueHead(filters = 1, kernel_size = (1, 1))\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "value_model.compile(optimizer = 'adam', loss = 'mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493ce585-acc9-4548-8a0d-964b9b77c12a",
   "metadata": {},
   "source": [
    "CALLBACK FUNCTION FOR SAVING POLICY MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cea10bed-02be-4e34-b25a-f3541149da0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_checkpoint_filepath = 'policy_model_res11.h5'\n",
    "policy_model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = policy_checkpoint_filepath,\n",
    "    save_weights_only = False,  # Save the entire model, including architecture and optimizer state\n",
    "    save_freq = 10000,          # Save every 10000 batches\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a720b0f1-7eb0-47d4-8979-b402352f1d5e",
   "metadata": {},
   "source": [
    "CALLBACK FUNCTION FOR SAVING VALUE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13763cf5-9b97-4828-b8cf-8f59737e6fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_checkpoint_filepath = 'value_model_res12.h5'\n",
    "value_model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = value_checkpoint_filepath,\n",
    "    save_weights_only = False,  # Save the entire model, including architecture and optimizer state\n",
    "    save_freq = 1000,          # Save every 10000 batches\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b5aaaed-a6fe-4e09-a526-8db8e104264a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193131"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4550e91-6c1a-4652-8b28-be508c24bb13",
   "metadata": {},
   "source": [
    "TRAINING THE VALUE NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d88906f5-e2af-4b02-a5de-1d0ba220b560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 283/6035 [>.............................] - ETA: 3:05:13 - loss: 0.6828"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m steps_per_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(value_ind_train) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size\n\u001b[0;32m      3\u001b[0m value_train_data_generator \u001b[38;5;241m=\u001b[39m value_data_generator(value_fen, values, value_ind_train, batch_size)\n\u001b[1;32m----> 4\u001b[0m value_model\u001b[38;5;241m.\u001b[39mfit(value_train_data_generator,\n\u001b[0;32m      5\u001b[0m           epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m      6\u001b[0m           steps_per_epoch \u001b[38;5;241m=\u001b[39m steps_per_epoch,\n\u001b[0;32m      7\u001b[0m           callbacks \u001b[38;5;241m=\u001b[39m [value_model_checkpoint_callback])\n\u001b[0;32m      8\u001b[0m value_train_data_generator\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ddp\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ddp\\Lib\\site-packages\\keras\\src\\engine\\training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1740\u001b[0m ):\n\u001b[0;32m   1741\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1742\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m   1743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1744\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ddp\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ddp\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ddp\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ddp\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concrete_function\u001b[38;5;241m.\u001b[39m_call_flat(\n\u001b[0;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[38;5;241m=\u001b[39mconcrete_function\u001b[38;5;241m.\u001b[39mcaptured_inputs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ddp\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function(\u001b[38;5;241m*\u001b[39margs))\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ddp\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    200\u001b[0m     )\n\u001b[0;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ddp\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1458\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1459\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1460\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1461\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1462\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1463\u001b[0m   )\n\u001b[0;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ddp\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "steps_per_epoch = len(value_ind_train) // batch_size\n",
    "value_train_data_generator = value_data_generator(value_fen, values, value_ind_train, batch_size)\n",
    "value_model.fit(value_train_data_generator,\n",
    "          epochs = 1,\n",
    "          steps_per_epoch = steps_per_epoch,\n",
    "          callbacks = [value_model_checkpoint_callback])\n",
    "value_train_data_generator.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068ab492-203c-4f52-86e4-a032528561a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_model.save('value_model_res12.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0710a155-30fe-4472-8854-ad2a3373ffd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system(\"shutdown /s /t 1\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba22419-7d1e-495c-9413-5ded64a58395",
   "metadata": {
    "id": "dba22419-7d1e-495c-9413-5ded64a58395"
   },
   "source": [
    "TRAINING THE POLICY NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cde9ad0-a0d2-41d7-b377-e84bcf9cc9fc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8cde9ad0-a0d2-41d7-b377-e84bcf9cc9fc",
    "outputId": "c59a80c3-8eb9-4e79-929b-46f0ba73e006"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "steps_per_epoch = len(ind_train) // batch_size\n",
    "train_data_generator = policy_data_generator(fen, action, ind_train, game_nos, pre_sum, all_fen, batch_size, history = 0)\n",
    "policy_model.fit(train_data_generator,\n",
    "          epochs = 1,\n",
    "          steps_per_epoch = steps_per_epoch,\n",
    "          callbacks = [policy_model_checkpoint_callback])\n",
    "train_data_generator.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8243069-f660-4ccc-becb-db96434f92a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3d8424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_best_move_hist(model, fen, history = 3):\n",
    "    enc_fen = parse_fen(fen)\n",
    "    stack_fen = np.tile(enc_fen, (1, 1, history + 1))\n",
    "    out_policy = model.predict(np.array([stack_fen]))[0].reshape((8, 8, 73))\n",
    "    return out_policy, decode_action(out_policy, fen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e04fd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_fen = 'rn2kb1r/pp3ppp/4p1qn/1p4B1/2B5/3P2QP/PPP2PP1/R3K2R w - - 1 0'\n",
    "b = chess.Board(samp_fen)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0602872e",
   "metadata": {},
   "outputs": [],
   "source": [
    "out, p = play_best_move_hist(model, samp_fen, history = 0)\n",
    "p[0]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
