{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a14f920e-db96-4fcb-945a-de40891eff51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import catboost\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, balanced_accuracy_score, precision_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53c4b451-8b49-4e6b-a92e-0e09a17f467c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fens</th>\n",
       "      <th>plies</th>\n",
       "      <th>w_elo</th>\n",
       "      <th>b_elo</th>\n",
       "      <th>w_time</th>\n",
       "      <th>b_time</th>\n",
       "      <th>inc_time</th>\n",
       "      <th>stock_evals</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rn3r1k/pp3ppp/4p3/3p1N2/8/2NP2PP/PPP3P1/2KR3R ...</td>\n",
       "      <td>34</td>\n",
       "      <td>2059</td>\n",
       "      <td>1941</td>\n",
       "      <td>833.51</td>\n",
       "      <td>559.17</td>\n",
       "      <td>10.00</td>\n",
       "      <td>{'e6f5': 239, 'h8g8': 458, 'b8c6': 476, 'h7h6'...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3r3k/1p3ppp/p1n1r3/3N1p2/1PP5/P2P2PP/3K2P1/4R2...</td>\n",
       "      <td>46</td>\n",
       "      <td>2059</td>\n",
       "      <td>1941</td>\n",
       "      <td>781.44</td>\n",
       "      <td>483.19</td>\n",
       "      <td>10.00</td>\n",
       "      <td>{'g7g5': 232, 'h8g8': 253, 'b7b5': 254, 'g7g6'...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4k3/1p6/p5pp/3p1p2/1PP4P/P1K3P1/6P1/8 w - - 0 34</td>\n",
       "      <td>67</td>\n",
       "      <td>2059</td>\n",
       "      <td>1941</td>\n",
       "      <td>702.54</td>\n",
       "      <td>307.39</td>\n",
       "      <td>10.00</td>\n",
       "      <td>{'c4d5': 206, 'c3d4': 131, 'c3d3': 130, 'a3a4'...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8/1p6/p2k2pp/3P1p2/PP5P/2K3P1/6P1/8 w - - 1 36</td>\n",
       "      <td>71</td>\n",
       "      <td>2059</td>\n",
       "      <td>1941</td>\n",
       "      <td>684.55</td>\n",
       "      <td>316.30</td>\n",
       "      <td>10.00</td>\n",
       "      <td>{'c3c4': 284, 'h4h5': 283, 'c3d4': 229, 'g3g4'...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>r2q1rk1/1pp1b1pp/2np4/p7/2Pp4/P4BPP/1P1BQP2/4R...</td>\n",
       "      <td>36</td>\n",
       "      <td>2743</td>\n",
       "      <td>2319</td>\n",
       "      <td>602.32</td>\n",
       "      <td>790.94</td>\n",
       "      <td>0.00</td>\n",
       "      <td>{'e7f6': 34, 'f8e8': 72, 'e7g5': 82, 'f8f3': 1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                fens  plies  w_elo  b_elo  \\\n",
       "0  rn3r1k/pp3ppp/4p3/3p1N2/8/2NP2PP/PPP3P1/2KR3R ...     34   2059   1941   \n",
       "1  3r3k/1p3ppp/p1n1r3/3N1p2/1PP5/P2P2PP/3K2P1/4R2...     46   2059   1941   \n",
       "2   4k3/1p6/p5pp/3p1p2/1PP4P/P1K3P1/6P1/8 w - - 0 34     67   2059   1941   \n",
       "3     8/1p6/p2k2pp/3P1p2/PP5P/2K3P1/6P1/8 w - - 1 36     71   2059   1941   \n",
       "4  r2q1rk1/1pp1b1pp/2np4/p7/2Pp4/P4BPP/1P1BQP2/4R...     36   2743   2319   \n",
       "\n",
       "   w_time  b_time  inc_time  \\\n",
       "0  833.51  559.17     10.00   \n",
       "1  781.44  483.19     10.00   \n",
       "2  702.54  307.39     10.00   \n",
       "3  684.55  316.30     10.00   \n",
       "4  602.32  790.94      0.00   \n",
       "\n",
       "                                         stock_evals  result  \n",
       "0  {'e6f5': 239, 'h8g8': 458, 'b8c6': 476, 'h7h6'...       1  \n",
       "1  {'g7g5': 232, 'h8g8': 253, 'b7b5': 254, 'g7g6'...       1  \n",
       "2  {'c4d5': 206, 'c3d4': 131, 'c3d3': 130, 'a3a4'...       1  \n",
       "3  {'c3c4': 284, 'h4h5': 283, 'c3d4': 229, 'g3g4'...       1  \n",
       "4  {'e7f6': 34, 'f8e8': 72, 'e7g5': 82, 'f8f3': 1...       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('game_data_final_win_loss.csv')\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "cf3b9c23-3d9d-40a2-8053-2855e8e97d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'e6f5': 239, 'h8g8': 458, 'b8c6': 476, 'h7h6': 477, 'b8a6': 481, 'b7b5': 481, 'f7f6': 489, 'a7a5': 496, 'f8d8': 501, 'f8g8': 502, 'f8e8': 502, 'g7g6': 503, 'a7a6': 506, 'f8c8': 509, 'b7b6': 509, 'b8d7': 512, 'h7h5': 525, 'g7g5': 573, 'e6e5': 624, 'd5d4': 614}\n",
      "0.5881818181818181\n",
      "[  0 219 237 238 242 242 250 257 262 263 263 264 267 270 270 273 286 334\n",
      " 385 375]\n",
      "[9.59500938e-01 5.55079170e-03 3.63442043e-03 3.54991047e-03\n",
      " 3.23106981e-03 3.23106981e-03 2.67672789e-03 2.27028234e-03\n",
      " 2.01832476e-03 1.97139333e-03 1.97139333e-03 1.92555318e-03\n",
      " 1.79432961e-03 1.67204873e-03 1.67204873e-03 1.55810110e-03\n",
      " 1.14753171e-03 3.70947106e-04 1.11739310e-04 1.41378586e-04]\n",
      "1.0422084651584114\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "samp_eval = ast.literal_eval(data['stock_evals'].values[idx])\n",
    "print(samp_eval)\n",
    "move_evals = np.array(list(samp_eval.values()))\n",
    "loss = np.abs(move_evals - move_evals[0])\n",
    "is_white = plies[idx] % 2\n",
    "alpha = (is_white*w_elos[idx] + (1 - is_white)*b_elos[idx])/3300\n",
    "print(alpha)\n",
    "expo_loss = np.exp(-4*alpha*0.01*loss)\n",
    "print(loss)\n",
    "print(expo_loss/sum(expo_loss))\n",
    "#print(len(expo_loss))\n",
    "total_expo_loss = np.sum(expo_loss)\n",
    "expected_loss = np.dot(expo_loss, loss)/total_expo_loss\n",
    "print(total_expo_loss)\n",
    "print(len(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2c38dcd6-348e-4809-b1f8-3e359ee1f906",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_evals = data['stock_evals'].values\n",
    "w_elos = data['w_elo'].values\n",
    "b_elos = data['b_elo'].values\n",
    "plies = data['plies'].values\n",
    "w_plus_all = []\n",
    "b_plus_all = []\n",
    "equals_all = []\n",
    "pos_evals = []\n",
    "expected_losses = []\n",
    "std_losses = []\n",
    "thres = 20\n",
    "f = 4\n",
    "for i, eval in enumerate(stock_evals):\n",
    "    dict = ast.literal_eval(eval)\n",
    "    move_evals = np.array(list(dict.values()))\n",
    "    pos_evals.append(move_evals[0])\n",
    "    loss = np.abs(move_evals - move_evals[0])\n",
    "    is_white = plies[i] % 2\n",
    "    alpha = (is_white*w_elos[i] + (1 - is_white)*b_elos[i])/3000\n",
    "    expo_loss = np.exp(-f*alpha*0.01*loss)\n",
    "    total_expo_loss = np.sum(expo_loss)\n",
    "    expected_loss = np.dot(expo_loss, loss)/total_expo_loss\n",
    "    expected_losses.append(expected_loss)\n",
    "    std_losses.append(np.sqrt(np.dot(expo_loss, loss*loss)/total_expo_loss- expected_loss*expected_loss))\n",
    "    w_plus = b_plus = equals = 0\n",
    "    for item in dict.values():\n",
    "        if item > thres:\n",
    "            w_plus += 1\n",
    "        elif item < -thres:\n",
    "            b_plus += 1\n",
    "        else:\n",
    "            equals += 1\n",
    "    w_plus_all.append(w_plus)\n",
    "    b_plus_all.append(b_plus)\n",
    "    equals_all.append(equals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a6885ffd-4e72-40a9-a1cc-ae2bdeb3ed96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>plies</th>\n",
       "      <th>w_elo</th>\n",
       "      <th>b_elo</th>\n",
       "      <th>w_time</th>\n",
       "      <th>b_time</th>\n",
       "      <th>inc_time</th>\n",
       "      <th>expected_losses</th>\n",
       "      <th>std_losses</th>\n",
       "      <th>pos_evals</th>\n",
       "      <th>white_plus</th>\n",
       "      <th>black_plus</th>\n",
       "      <th>equals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34</td>\n",
       "      <td>2059</td>\n",
       "      <td>1941</td>\n",
       "      <td>833.51</td>\n",
       "      <td>559.17</td>\n",
       "      <td>10.00</td>\n",
       "      <td>5.72</td>\n",
       "      <td>37.51</td>\n",
       "      <td>239</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46</td>\n",
       "      <td>2059</td>\n",
       "      <td>1941</td>\n",
       "      <td>781.44</td>\n",
       "      <td>483.19</td>\n",
       "      <td>10.00</td>\n",
       "      <td>42.47</td>\n",
       "      <td>34.43</td>\n",
       "      <td>232</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "      <td>2059</td>\n",
       "      <td>1941</td>\n",
       "      <td>702.54</td>\n",
       "      <td>307.39</td>\n",
       "      <td>10.00</td>\n",
       "      <td>30.10</td>\n",
       "      <td>43.83</td>\n",
       "      <td>206</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71</td>\n",
       "      <td>2059</td>\n",
       "      <td>1941</td>\n",
       "      <td>684.55</td>\n",
       "      <td>316.30</td>\n",
       "      <td>10.00</td>\n",
       "      <td>6.09</td>\n",
       "      <td>17.35</td>\n",
       "      <td>284</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36</td>\n",
       "      <td>2743</td>\n",
       "      <td>2319</td>\n",
       "      <td>602.32</td>\n",
       "      <td>790.94</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17.85</td>\n",
       "      <td>28.57</td>\n",
       "      <td>34</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   plies  w_elo  b_elo  w_time  b_time  inc_time  expected_losses  std_losses  \\\n",
       "0     34   2059   1941  833.51  559.17     10.00             5.72       37.51   \n",
       "1     46   2059   1941  781.44  483.19     10.00            42.47       34.43   \n",
       "2     67   2059   1941  702.54  307.39     10.00            30.10       43.83   \n",
       "3     71   2059   1941  684.55  316.30     10.00             6.09       17.35   \n",
       "4     36   2743   2319  602.32  790.94      0.00            17.85       28.57   \n",
       "\n",
       "   pos_evals  white_plus  black_plus  equals  \n",
       "0        239          20           0       0  \n",
       "1        232          37           0       0  \n",
       "2        206           8           3       1  \n",
       "3        284           3           7       1  \n",
       "4         34          32           0       0  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_x = data.iloc[:, 1:-2]\n",
    "data_x['expected_losses'] = expected_losses\n",
    "data_x['std_losses'] = std_losses\n",
    "data_x['pos_evals'] = pos_evals\n",
    "data_x['white_plus'] = w_plus_all\n",
    "data_x['black_plus'] = b_plus_all\n",
    "data_x['equals'] = equals_all\n",
    "data_x.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "02da934c-0024-4b19-a149-c41316fa0cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_x.values\n",
    "y = data['result'].values + 1\n",
    "# x_train, x_test_val, y_train, y_test_val = train_test_split(x, y, test_size = 0.3, shuffle = True, random_state = 0)\n",
    "# x_val, x_test, y_val, y_test = train_test_split(x_test_val, y_test_val, test_size = 1.0)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, shuffle = True, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07786e6a-28d5-4e2d-8ae0-a8b6b31638fd",
   "metadata": {},
   "source": [
    "BASIC STOCKFISH BASED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d921c03c-5634-478c-b492-fdd0270e3a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 71.9184468008811%\n",
      "Balanced Accuracy = 68.8888913314421%\n",
      "Precision = 74.85438224187483%\n",
      "F1 Score = 72.9310289797178%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[27769,  6954,  4488],\n",
       "       [ 2619,  9058,  4026],\n",
       "       [ 2902,  6420, 33369]], dtype=int64)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bd = 50 #equality threshold\n",
    "test_evals = x_test[:, 8]\n",
    "y_pred_stock = np.array([it > bd for it in test_evals]).astype(int) - np.array([it < -bd for it in test_evals]).astype(int) + 1\n",
    "#y_true = ohe.inverse_transform(y_test).reshape(-1)\n",
    "print(f'Accuracy = {accuracy_score(y_test, y_pred_stock)*100}%')\n",
    "print(f'Balanced Accuracy = {balanced_accuracy_score(y_test, y_pred_stock)*100}%')\n",
    "print(f'Precision = {precision_score(y_test, y_pred_stock, average = mode)*100}%')\n",
    "print(f'F1 Score = {f1_score(y_test, y_pred_stock, average = mode)*100}%')\n",
    "confusion_matrix(y_test, y_pred_stock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1fcd056a-fa85-4771-9796-055a8cd4b591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0, 1, 2], dtype=int64)]\n"
     ]
    }
   ],
   "source": [
    "ohe = OneHotEncoder(sparse_output = False)\n",
    "y_ohe = ohe.fit_transform(y.reshape(-1, 1))\n",
    "print(ohe.categories_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0d3026e1-7bc6-4a9c-9efa-c5b7e7f6cc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a2b15b6d-2113-4cbf-844d-1b1169819ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_nn, x_test_val_nn, y_train_nn, y_test_val_nn = train_test_split(x, y_ohe, test_size = 0.3, shuffle = True, random_state = 0)\n",
    "x_val_nn, x_test_nn, y_val_nn, y_test_nn = train_test_split(x_test_val_nn, y_test_val_nn, shuffle = False, test_size = 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6692140c-38e0-4179-8c7d-be66d71b74b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ni = len(x_train[0])\n",
    "no = len(y_ohe[0])\n",
    "model_nn = Sequential([\n",
    "    Dense(256, input_shape = (ni,), activation = 'relu'),\n",
    "    #Dropout(0.1),\n",
    "    Dense(256, activation = 'relu'),\n",
    "    #Dropout(0.1),\n",
    "    #Dense(400, activation = 'relu'),\n",
    "    Dense(256, activation = 'relu'),\n",
    "    Dense(no, activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ba155286-08f0-4fec-bce8-6d7c153d7dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nn.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cbf12341-9fab-431e-af9a-4a2e7fcca794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "7117/7117 [==============================] - 21s 3ms/step - loss: 1.3347 - accuracy: 0.7313 - val_loss: 0.6138 - val_accuracy: 0.7612\n",
      "Epoch 2/50\n",
      "7117/7117 [==============================] - 21s 3ms/step - loss: 0.6050 - accuracy: 0.7574 - val_loss: 0.6071 - val_accuracy: 0.7357\n",
      "Epoch 3/50\n",
      "7117/7117 [==============================] - 24s 3ms/step - loss: 0.5796 - accuracy: 0.7618 - val_loss: 0.5713 - val_accuracy: 0.7699\n",
      "Epoch 4/50\n",
      "7117/7117 [==============================] - 22s 3ms/step - loss: 0.5722 - accuracy: 0.7663 - val_loss: 0.5603 - val_accuracy: 0.7738\n",
      "Epoch 5/50\n",
      "7117/7117 [==============================] - 20s 3ms/step - loss: 0.5696 - accuracy: 0.7676 - val_loss: 0.5712 - val_accuracy: 0.7728\n",
      "Epoch 6/50\n",
      "7117/7117 [==============================] - 21s 3ms/step - loss: 0.5681 - accuracy: 0.7702 - val_loss: 0.5654 - val_accuracy: 0.7624\n",
      "Epoch 7/50\n",
      "7117/7117 [==============================] - 21s 3ms/step - loss: 0.5661 - accuracy: 0.7723 - val_loss: 0.5721 - val_accuracy: 0.7646\n",
      "Epoch 8/50\n",
      "7117/7117 [==============================] - 22s 3ms/step - loss: 0.5618 - accuracy: 0.7749 - val_loss: 0.5536 - val_accuracy: 0.7750\n",
      "Epoch 9/50\n",
      "7117/7117 [==============================] - 20s 3ms/step - loss: 0.5513 - accuracy: 0.7792 - val_loss: 0.5676 - val_accuracy: 0.7705\n",
      "Epoch 10/50\n",
      "7117/7117 [==============================] - 20s 3ms/step - loss: 0.5532 - accuracy: 0.7804 - val_loss: 0.5482 - val_accuracy: 0.7855\n",
      "Epoch 11/50\n",
      "7117/7117 [==============================] - 22s 3ms/step - loss: 0.5475 - accuracy: 0.7840 - val_loss: 0.5334 - val_accuracy: 0.7875\n",
      "Epoch 12/50\n",
      "7117/7117 [==============================] - 23s 3ms/step - loss: 0.5440 - accuracy: 0.7859 - val_loss: 0.5373 - val_accuracy: 0.7888\n",
      "Epoch 13/50\n",
      "7117/7117 [==============================] - 23s 3ms/step - loss: 0.5364 - accuracy: 0.7855 - val_loss: 0.5301 - val_accuracy: 0.7867\n",
      "Epoch 14/50\n",
      "7117/7117 [==============================] - 23s 3ms/step - loss: 0.5340 - accuracy: 0.7873 - val_loss: 0.5273 - val_accuracy: 0.7873\n",
      "Epoch 15/50\n",
      "7117/7117 [==============================] - 23s 3ms/step - loss: 0.5327 - accuracy: 0.7865 - val_loss: 0.5315 - val_accuracy: 0.7887\n",
      "Epoch 16/50\n",
      "7117/7117 [==============================] - 23s 3ms/step - loss: 0.5407 - accuracy: 0.7868 - val_loss: 0.5395 - val_accuracy: 0.7852\n",
      "Epoch 17/50\n",
      "7117/7117 [==============================] - 24s 3ms/step - loss: 0.5322 - accuracy: 0.7879 - val_loss: 0.5298 - val_accuracy: 0.7873\n",
      "Epoch 18/50\n",
      "7117/7117 [==============================] - 21s 3ms/step - loss: 0.5311 - accuracy: 0.7874 - val_loss: 0.5270 - val_accuracy: 0.7884\n",
      "Epoch 19/50\n",
      "7117/7117 [==============================] - 23s 3ms/step - loss: 0.5312 - accuracy: 0.7878 - val_loss: 0.5307 - val_accuracy: 0.7863\n",
      "Epoch 20/50\n",
      "7117/7117 [==============================] - 23s 3ms/step - loss: 0.5301 - accuracy: 0.7887 - val_loss: 0.5253 - val_accuracy: 0.7902\n",
      "Epoch 21/50\n",
      "7117/7117 [==============================] - 35s 5ms/step - loss: 0.5303 - accuracy: 0.7881 - val_loss: 0.5316 - val_accuracy: 0.7863\n",
      "Epoch 22/50\n",
      "7117/7117 [==============================] - 43s 6ms/step - loss: 0.5284 - accuracy: 0.7881 - val_loss: 0.5259 - val_accuracy: 0.7878\n",
      "Epoch 23/50\n",
      "7117/7117 [==============================] - 42s 6ms/step - loss: 0.5286 - accuracy: 0.7890 - val_loss: 0.5322 - val_accuracy: 0.7831\n",
      "Epoch 24/50\n",
      "7117/7117 [==============================] - 40s 6ms/step - loss: 0.5270 - accuracy: 0.7886 - val_loss: 0.5326 - val_accuracy: 0.7885\n",
      "Epoch 25/50\n",
      "7117/7117 [==============================] - 40s 6ms/step - loss: 0.5305 - accuracy: 0.7891 - val_loss: 0.5258 - val_accuracy: 0.7898\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2bd3f3fb250>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 5)\n",
    "model_nn.fit(x_train_nn, y_train_nn, epochs = 50, validation_data = (x_val_nn, y_val_nn), batch_size = 32, callbacks = [early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d305f905-9739-4b8e-a916-0d44c83b515a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1831/1831 [==============================] - 6s 3ms/step\n",
      "Accuracy = 79.03795912094668%\n",
      "Balanced Accuracy = 72.56765094319591%\n",
      "Precision = 78.35163712628456%\n",
      "F1 Score = 78.45084774045826%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[19785,  1262,  2515],\n",
       "       [ 2123,  4492,  2801],\n",
       "       [ 2343,  1232, 22010]], dtype=int64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mode = 'weighted'\n",
    "y_pred_nn = model_nn.predict(x_test_nn)\n",
    "y_pred_nn = np.argmax(y_pred_nn, axis = 1)\n",
    "y_true_nn = ohe.inverse_transform(y_test_nn).reshape(-1)\n",
    "print(f'Accuracy = {accuracy_score(y_true_nn, y_pred_nn)*100}%')\n",
    "print(f'Balanced Accuracy = {balanced_accuracy_score(y_true_nn, y_pred_nn)*100}%')\n",
    "print(f'Precision = {precision_score(y_true_nn, y_pred_nn, average = mode)*100}%')\n",
    "print(f'F1 Score = {f1_score(y_true_nn, y_pred_nn, average = mode)*100}%')\n",
    "confusion_matrix(y_true_nn, y_pred_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3b0cf683-3e17-4fd9-9274-2e84de2c56de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nn.save('model_nn.keras')\n",
    "model_nn.save('model_nn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "1471f641-7c4b-42cf-b66c-ad01fd5f618d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# x_train_scaled = scaler.fit_transform(x_train)\n",
    "# x_val_scaled = scaler.transform(x_val)\n",
    "# x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c689ce7-cbfb-4b5d-a871-18ee4d33a54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cat = CatBoostClassifier(iterations = 5000, learning_rate = 0.05, depth = 6, eval_metric = 'Accuracy', random_seed = 42, logging_level = 'Verbose', use_best_model = True, early_stopping_rounds = 300)\n",
    "model_cat.fit(x_train_scaled, y_train, eval_set = (x_val_scaled, y_val))\n",
    "y_pred_cat = model_cat.predict(x_test_scaled)\n",
    "print(f'The accuracy of the catboost model is: {accuracy_score(y_test, y_pred_cat)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2887d250-6a31-437c-8301-825595ffdf98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▊                                                                                | 1/29 [00:27<12:37, 27.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC couldn't be calculated for AdaBoostClassifier\n",
      "multi_class must be in ('ovo', 'ovr')\n",
      "{'Model': 'AdaBoostClassifier', 'Accuracy': 0.7789730717347131, 'Balanced Accuracy': 0.7229667847417202, 'ROC AUC': None, 'F1 Score': 0.775764997315236, 'Time taken': 27.040196180343628}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████▋                                                                             | 2/29 [01:21<19:28, 43.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC couldn't be calculated for BaggingClassifier\n",
      "multi_class must be in ('ovo', 'ovr')\n",
      "{'Model': 'BaggingClassifier', 'Accuracy': 0.7871181462698291, 'Balanced Accuracy': 0.7338374775598187, 'ROC AUC': None, 'F1 Score': 0.7842639612468562, 'Time taken': 54.65120458602905}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████▌                                                                          | 3/29 [01:22<10:16, 23.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC couldn't be calculated for BernoulliNB\n",
      "multi_class must be in ('ovo', 'ovr')\n",
      "{'Model': 'BernoulliNB', 'Accuracy': 0.710533954886191, 'Balanced Accuracy': 0.6637997422840076, 'ROC AUC': None, 'F1 Score': 0.7140278833665981, 'Time taken': 0.3941528797149658}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|██████████████▎                                                                    | 5/29 [01:35<05:02, 12.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC couldn't be calculated for CalibratedClassifierCV\n",
      "multi_class must be in ('ovo', 'ovr')\n",
      "{'Model': 'CalibratedClassifierCV', 'Accuracy': 0.7665761658384987, 'Balanced Accuracy': 0.6751969113050826, 'ROC AUC': None, 'F1 Score': 0.7515222418435766, 'Time taken': 13.348979711532593}\n",
      "CategoricalNB model failed to execute\n",
      "Negative values in data passed to CategoricalNB (input X)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|████████████████████                                                               | 7/29 [01:42<02:39,  7.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC couldn't be calculated for DecisionTreeClassifier\n",
      "multi_class must be in ('ovo', 'ovr')\n",
      "{'Model': 'DecisionTreeClassifier', 'Accuracy': 0.7260898519543056, 'Balanced Accuracy': 0.6772103821871527, 'ROC AUC': None, 'F1 Score': 0.7265419265015002, 'Time taken': 6.947274208068848}\n",
      "ROC AUC couldn't be calculated for DummyClassifier\n",
      "multi_class must be in ('ovo', 'ovr')\n",
      "{'Model': 'DummyClassifier', 'Accuracy': 0.4368799412598398, 'Balanced Accuracy': 0.3333333333333333, 'ROC AUC': None, 'F1 Score': 0.2656646217885868, 'Time taken': 0.19817018508911133}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██████████████████████▉                                                            | 8/29 [01:43<01:47,  5.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC couldn't be calculated for ExtraTreeClassifier\n",
      "multi_class must be in ('ovo', 'ovr')\n",
      "{'Model': 'ExtraTreeClassifier', 'Accuracy': 0.6979150658265457, 'Balanced Accuracy': 0.6446792108955096, 'ROC AUC': None, 'F1 Score': 0.6979135302475048, 'Time taken': 0.6026477813720703}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|█████████████████████████▊                                                         | 9/29 [02:27<05:43, 17.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC couldn't be calculated for ExtraTreesClassifier\n",
      "multi_class must be in ('ovo', 'ovr')\n",
      "{'Model': 'ExtraTreesClassifier', 'Accuracy': 0.7939654730802725, 'Balanced Accuracy': 0.733598056959376, 'ROC AUC': None, 'F1 Score': 0.7889895637295378, 'Time taken': 43.641916275024414}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|████████████████████████████▎                                                     | 10/29 [02:27<03:47, 11.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC couldn't be calculated for GaussianNB\n",
      "multi_class must be in ('ovo', 'ovr')\n",
      "{'Model': 'GaussianNB', 'Accuracy': 0.6503765175964346, 'Balanced Accuracy': 0.6522282664556333, 'ROC AUC': None, 'F1 Score': 0.660831892558277, 'Time taken': 0.3610420227050781}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|█████████████████████████████████▉                                                | 12/29 [03:37<05:51, 20.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC couldn't be calculated for KNeighborsClassifier\n",
      "multi_class must be in ('ovo', 'ovr')\n",
      "{'Model': 'KNeighborsClassifier', 'Accuracy': 0.7460512610351245, 'Balanced Accuracy': 0.6876751228913638, 'ROC AUC': None, 'F1 Score': 0.7430271451181568, 'Time taken': 69.66015887260437}\n",
      "LabelPropagation model failed to execute\n",
      "Unable to allocate 386. GiB for an array with shape (227743, 227743) and data type float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████████████████████████████████████▊                                             | 13/29 [03:37<03:51, 14.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LabelSpreading model failed to execute\n",
      "Unable to allocate 386. GiB for an array with shape (227743, 227743) and data type float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|███████████████████████████████████████▌                                          | 14/29 [03:38<02:34, 10.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC couldn't be calculated for LinearDiscriminantAnalysis\n",
      "multi_class must be in ('ovo', 'ovr')\n",
      "{'Model': 'LinearDiscriminantAnalysis', 'Accuracy': 0.7142222905247341, 'Balanced Accuracy': 0.6541071000469287, 'ROC AUC': None, 'F1 Score': 0.7109406169542661, 'Time taken': 0.660679817199707}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|██████████████████████████████████████████▍                                       | 15/29 [06:45<14:51, 63.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC couldn't be calculated for LinearSVC\n",
      "multi_class must be in ('ovo', 'ovr')\n",
      "{'Model': 'LinearSVC', 'Accuracy': 0.7673445690965285, 'Balanced Accuracy': 0.6710809136119464, 'ROC AUC': None, 'F1 Score': 0.7497792190074554, 'Time taken': 187.33356928825378}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████████████████████████████████████████████▏                                    | 16/29 [06:47<09:45, 45.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC couldn't be calculated for LogisticRegression\n",
      "multi_class must be in ('ovo', 'ovr')\n",
      "{'Model': 'LogisticRegression', 'Accuracy': 0.7770606014036167, 'Balanced Accuracy': 0.7011862556132931, 'ROC AUC': None, 'F1 Score': 0.7678934016038074, 'Time taken': 1.8260858058929443}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|████████████████████████████████████████████████                                  | 17/29 [06:47<06:19, 31.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC couldn't be calculated for NearestCentroid\n",
      "multi_class must be in ('ovo', 'ovr')\n",
      "{'Model': 'NearestCentroid', 'Accuracy': 0.6480030053105202, 'Balanced Accuracy': 0.6507328830491853, 'ROC AUC': None, 'F1 Score': 0.6608982187891872, 'Time taken': 0.48677706718444824}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|███████████████████████████████████████████████▏                            | 18/29 [1:21:48<4:12:01, 1374.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC couldn't be calculated for NuSVC\n",
      "multi_class must be in ('ovo', 'ovr')\n",
      "{'Model': 'NuSVC', 'Accuracy': 0.7544866212454963, 'Balanced Accuracy': 0.6684938566147175, 'ROC AUC': None, 'F1 Score': 0.7416165675719839, 'Time taken': 4501.061406135559}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████████████████████████████████████████████████▍                          | 19/29 [1:21:49<2:40:21, 962.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC couldn't be calculated for PassiveAggressiveClassifier\n",
      "multi_class must be in ('ovo', 'ovr')\n",
      "{'Model': 'PassiveAggressiveClassifier', 'Accuracy': 0.6508375595512524, 'Balanced Accuracy': 0.5416586896665511, 'ROC AUC': None, 'F1 Score': 0.6199930824952864, 'Time taken': 1.1004793643951416}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|█████████████████████████████████████████████████████                        | 20/29 [1:21:50<1:41:02, 673.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC couldn't be calculated for Perceptron\n",
      "multi_class must be in ('ovo', 'ovr')\n",
      "{'Model': 'Perceptron', 'Accuracy': 0.6766900602769667, 'Balanced Accuracy': 0.613668896076613, 'ROC AUC': None, 'F1 Score': 0.6731933877945949, 'Time taken': 1.0847251415252686}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████████████████████████████████████████████████████▊                     | 21/29 [1:21:51<1:02:52, 471.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC couldn't be calculated for QuadraticDiscriminantAnalysis\n",
      "multi_class must be in ('ovo', 'ovr')\n",
      "{'Model': 'QuadraticDiscriminantAnalysis', 'Accuracy': 0.6904017895258098, 'Balanced Accuracy': 0.6926124175113939, 'ROC AUC': None, 'F1 Score': 0.7035517166392586, 'Time taken': 0.47107768058776855}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████████████████████████████████████████████████████████▉                   | 22/29 [1:24:16<43:34, 373.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC couldn't be calculated for RandomForestClassifier\n",
      "multi_class must be in ('ovo', 'ovr')\n",
      "{'Model': 'RandomForestClassifier', 'Accuracy': 0.8007444973788911, 'Balanced Accuracy': 0.7462065970985671, 'ROC AUC': None, 'F1 Score': 0.7972533828900241, 'Time taken': 145.03856658935547}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|██████████████████████████████████████████████████████████████▋                | 23/29 [1:24:16<26:09, 261.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC couldn't be calculated for RidgeClassifier\n",
      "multi_class must be in ('ovo', 'ovr')\n",
      "{'Model': 'RidgeClassifier', 'Accuracy': 0.718525348769701, 'Balanced Accuracy': 0.6262115046582433, 'ROC AUC': None, 'F1 Score': 0.7017237887661602, 'Time taken': 0.4085366725921631}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|█████████████████████████████████████████████████████████████████▍             | 24/29 [1:24:17<15:16, 183.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC couldn't be calculated for RidgeClassifierCV\n",
      "multi_class must be in ('ovo', 'ovr')\n",
      "{'Model': 'RidgeClassifierCV', 'Accuracy': 0.7185082731417448, 'Balanced Accuracy': 0.6261762573987177, 'ROC AUC': None, 'F1 Score': 0.70169884314487, 'Time taken': 0.5817487239837646}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████████████████████████████████████████████████████████████████           | 25/29 [1:24:19<08:35, 128.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC couldn't be calculated for SGDClassifier\n",
      "multi_class must be in ('ovo', 'ovr')\n",
      "{'Model': 'SGDClassifier', 'Accuracy': 0.7543500162218466, 'Balanced Accuracy': 0.623169986065126, 'ROC AUC': None, 'F1 Score': 0.7126541507892046, 'Time taken': 1.664670467376709}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|██████████████████████████████████████████████████████████████████████▊        | 26/29 [2:02:17<38:40, 773.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC couldn't be calculated for SVC\n",
      "multi_class must be in ('ovo', 'ovr')\n",
      "{'Model': 'SVC', 'Accuracy': 0.7912333726072777, 'Balanced Accuracy': 0.7313774535698546, 'ROC AUC': None, 'F1 Score': 0.7865251845019231, 'Time taken': 2278.0826377868652}\n",
      "StackingClassifier model failed to execute\n",
      "StackingClassifier.__init__() missing 1 required positional argument: 'estimators'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|████████████████████████████████████████████████████████████████████████████▎  | 28/29 [2:02:21<06:57, 417.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC couldn't be calculated for XGBClassifier\n",
      "multi_class must be in ('ovo', 'ovr')\n",
      "{'Model': 'XGBClassifier', 'Accuracy': 0.8010347830541468, 'Balanced Accuracy': 0.7485266373444378, 'ROC AUC': None, 'F1 Score': 0.7980761544871428, 'Time taken': 3.9556941986083984}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003888 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2245\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.911343\n",
      "[LightGBM] [Info] Start training from score -1.830111\n",
      "[LightGBM] [Info] Start training from score -0.826403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 29/29 [2:02:24<00:00, 253.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC couldn't be calculated for LGBMClassifier\n",
      "multi_class must be in ('ovo', 'ovr')\n",
      "{'Model': 'LGBMClassifier', 'Accuracy': 0.7987124976521012, 'Balanced Accuracy': 0.7455804498255884, 'ROC AUC': None, 'F1 Score': 0.7956645138742477, 'Time taken': 3.0260884761810303}\n",
      "                               Accuracy  Balanced Accuracy ROC AUC  F1 Score  \\\n",
      "Model                                                                          \n",
      "XGBClassifier                      0.80               0.75    None      0.80   \n",
      "RandomForestClassifier             0.80               0.75    None      0.80   \n",
      "LGBMClassifier                     0.80               0.75    None      0.80   \n",
      "BaggingClassifier                  0.79               0.73    None      0.78   \n",
      "ExtraTreesClassifier               0.79               0.73    None      0.79   \n",
      "SVC                                0.79               0.73    None      0.79   \n",
      "AdaBoostClassifier                 0.78               0.72    None      0.78   \n",
      "LogisticRegression                 0.78               0.70    None      0.77   \n",
      "QuadraticDiscriminantAnalysis      0.69               0.69    None      0.70   \n",
      "KNeighborsClassifier               0.75               0.69    None      0.74   \n",
      "DecisionTreeClassifier             0.73               0.68    None      0.73   \n",
      "CalibratedClassifierCV             0.77               0.68    None      0.75   \n",
      "LinearSVC                          0.77               0.67    None      0.75   \n",
      "NuSVC                              0.75               0.67    None      0.74   \n",
      "BernoulliNB                        0.71               0.66    None      0.71   \n",
      "LinearDiscriminantAnalysis         0.71               0.65    None      0.71   \n",
      "GaussianNB                         0.65               0.65    None      0.66   \n",
      "NearestCentroid                    0.65               0.65    None      0.66   \n",
      "ExtraTreeClassifier                0.70               0.64    None      0.70   \n",
      "RidgeClassifier                    0.72               0.63    None      0.70   \n",
      "RidgeClassifierCV                  0.72               0.63    None      0.70   \n",
      "SGDClassifier                      0.75               0.62    None      0.71   \n",
      "Perceptron                         0.68               0.61    None      0.67   \n",
      "PassiveAggressiveClassifier        0.65               0.54    None      0.62   \n",
      "DummyClassifier                    0.44               0.33    None      0.27   \n",
      "\n",
      "                               Time Taken  \n",
      "Model                                      \n",
      "XGBClassifier                        3.96  \n",
      "RandomForestClassifier             145.04  \n",
      "LGBMClassifier                       3.03  \n",
      "BaggingClassifier                   54.65  \n",
      "ExtraTreesClassifier                43.64  \n",
      "SVC                               2278.08  \n",
      "AdaBoostClassifier                  27.04  \n",
      "LogisticRegression                   1.83  \n",
      "QuadraticDiscriminantAnalysis        0.47  \n",
      "KNeighborsClassifier                69.66  \n",
      "DecisionTreeClassifier               6.95  \n",
      "CalibratedClassifierCV              13.35  \n",
      "LinearSVC                          187.33  \n",
      "NuSVC                             4501.06  \n",
      "BernoulliNB                          0.39  \n",
      "LinearDiscriminantAnalysis           0.66  \n",
      "GaussianNB                           0.36  \n",
      "NearestCentroid                      0.49  \n",
      "ExtraTreeClassifier                  0.60  \n",
      "RidgeClassifier                      0.41  \n",
      "RidgeClassifierCV                    0.58  \n",
      "SGDClassifier                        1.66  \n",
      "Perceptron                           1.08  \n",
      "PassiveAggressiveClassifier          1.10  \n",
      "DummyClassifier                      0.20  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LazyClassifier(verbose = 1, ignore_warnings = False, custom_metric = None)\n",
    "models, predictions = clf.fit(x_train, x_test, y_train, y_test)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "e57eb915-01c3-4b03-a583-a31c1b5a57be",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[184], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[0;32m      2\u001b[0m model_rf \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m, random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m model_rf\u001b[38;5;241m.\u001b[39mfit(x_train, y_train)\n\u001b[0;32m      4\u001b[0m y_pred_rf \u001b[38;5;241m=\u001b[39m model_rf\u001b[38;5;241m.\u001b[39mpredict(x_test)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe accuracy of the random forest model is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy_score(y_test,\u001b[38;5;250m \u001b[39my_pred_rf)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ddp\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ddp\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:456\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    445\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    448\u001b[0m ]\n\u001b[0;32m    450\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m trees \u001b[38;5;241m=\u001b[39m Parallel(\n\u001b[0;32m    457\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[0;32m    458\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    459\u001b[0m     prefer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreads\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    460\u001b[0m )(\n\u001b[0;32m    461\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    462\u001b[0m         t,\n\u001b[0;32m    463\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbootstrap,\n\u001b[0;32m    464\u001b[0m         X,\n\u001b[0;32m    465\u001b[0m         y,\n\u001b[0;32m    466\u001b[0m         sample_weight,\n\u001b[0;32m    467\u001b[0m         i,\n\u001b[0;32m    468\u001b[0m         \u001b[38;5;28mlen\u001b[39m(trees),\n\u001b[0;32m    469\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    470\u001b[0m         class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight,\n\u001b[0;32m    471\u001b[0m         n_samples_bootstrap\u001b[38;5;241m=\u001b[39mn_samples_bootstrap,\n\u001b[0;32m    472\u001b[0m     )\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trees)\n\u001b[0;32m    474\u001b[0m )\n\u001b[0;32m    476\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ddp\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ddp\\Lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ddp\\Lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ddp\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ddp\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:188\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    186\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 188\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39mcurr_sample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ddp\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ddp\\Lib\\site-packages\\sklearn\\tree\\_classes.py:959\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    930\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    931\u001b[0m \n\u001b[0;32m    932\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 959\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m    960\u001b[0m         X,\n\u001b[0;32m    961\u001b[0m         y,\n\u001b[0;32m    962\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m    963\u001b[0m         check_input\u001b[38;5;241m=\u001b[39mcheck_input,\n\u001b[0;32m    964\u001b[0m     )\n\u001b[0;32m    965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ddp\\Lib\\site-packages\\sklearn\\tree\\_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    434\u001b[0m         splitter,\n\u001b[0;32m    435\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    441\u001b[0m     )\n\u001b[1;32m--> 443\u001b[0m builder\u001b[38;5;241m.\u001b[39mbuild(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_, X, y, sample_weight, missing_values_in_feature_mask)\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_rf = RandomForestClassifier(n_estimators = 1000, random_state = 42)\n",
    "model_rf.fit(x_train, y_train)\n",
    "y_pred_rf = model_rf.predict(x_test)\n",
    "print(f'The accuracy of the random forest model is {accuracy_score(y_test, y_pred_rf)*100}%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "860a9978-1fd1-464a-ab9f-984eb4340167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\dell\\anaconda3\\envs\\ddp\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\dell\\anaconda3\\envs\\ddp\\lib\\site-packages (from xgboost) (1.24.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\dell\\anaconda3\\envs\\ddp\\lib\\site-packages (from xgboost) (1.11.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "769949fa-17a5-4900-9777-5f09845b0a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.3'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "xgb.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "67e0fd9e-7846-4ee8-95e5-16f9fee778db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the xgboost model is 81.55524819425234%.\n"
     ]
    }
   ],
   "source": [
    "model_xgb = xgb.XGBClassifier(\n",
    "    objective = 'multi:softprob',  # Change this based on your specific problem (e.g., 'multi:softprob' for multi-class classification)\n",
    "    n_estimators = 1000,\n",
    "    max_depth = 15,\n",
    "    learning_rate = 0.1,\n",
    "    use_label_encoder = False  # Set this to False if you're using XGBoost version >= 1.3.0\n",
    ")\n",
    "model_xgb.fit(x_train, y_train)\n",
    "y_pred_xgb = model_xgb.predict(x_test)\n",
    "print(f'The accuracy of the xgboost model is {accuracy_score(y_test, y_pred_xgb)*100}%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de50e8c-9097-495e-8b27-0b2ca24592c1",
   "metadata": {},
   "source": [
    "ENSEMBLING THE LIGHTGBM AND XGBOOST MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74019329-4dd9-41f9-a1a4-8d8889026e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_lgbm = model_lgbm_loaded.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b90b617a-56ac-457d-a743-023296ea5291",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_xgb = model_xgb_loaded.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d39855df-4863-476a-bb7e-808746830a6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1, ..., 0, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_combine = np.argmax((prob_lgbm + prob_xgb)/2, axis = 1)\n",
    "y_pred_combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8738a835-9580-4c89-8d3d-47128dc23ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the ensemble model\n",
      "Accuracy = 81.93125352184826%\n",
      "Balanced Accuracy = 76.96668233377673%\n",
      "Precision = 81.54778659006567%\n",
      "F1 Score = 81.64209162115128%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[33619,  1919,  3673],\n",
       "       [ 2980,  9089,  3634],\n",
       "       [ 3258,  2172, 37261]], dtype=int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('For the ensemble model')\n",
    "print(f'Accuracy = {accuracy_score(y_test, y_pred_combine)*100}%') #best final model. close analysis. combination of lightgbm and xgboost models\n",
    "print(f'Balanced Accuracy = {balanced_accuracy_score(y_test, y_pred_combine)*100}%')\n",
    "print(f'Precision = {precision_score(y_test, y_pred_combine, average = mode)*100}%')\n",
    "print(f'F1 Score = {f1_score(y_test, y_pred_combine, average = mode)*100}%')\n",
    "confusion_matrix(y_test, y_pred_combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "01edc7bc-986d-4ff0-b801-a52c781b2c38",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[219], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m prob_lgbm_train \u001b[38;5;241m=\u001b[39m model_lgbm\u001b[38;5;241m.\u001b[39mpredict_proba(x_train)\n\u001b[0;32m      2\u001b[0m prob_xgb_train \u001b[38;5;241m=\u001b[39m model_xgb\u001b[38;5;241m.\u001b[39mpredict_proba(x_train)\n\u001b[1;32m----> 3\u001b[0m x_new_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((np\u001b[38;5;241m.\u001b[39mconcatenate((prob_lgbm_train, prob_xgb_train), axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m), y_train), axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)"
     ]
    }
   ],
   "source": [
    "prob_lgbm_train = model_lgbm.predict_proba(x_train)\n",
    "prob_xgb_train = model_xgb.predict_proba(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "2bed2115-27e9-4088-b3ce-00dfb89e2a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new_train = np.concatenate((prob_lgbm_train, prob_xgb_train), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "d7c28b1f-a760-4a86-9067-0d15233641fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_comb_train, x_comb_val, y_comb_train, y_comb_val = train_test_split(x_new_train, y_train, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da285884-af15-481f-9417-ddb78e64374c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cat = CatBoostClassifier(iterations = 5000, learning_rate = 0.05, depth = 6, eval_metric = 'Accuracy', random_seed = 42, logging_level = 'Verbose', use_best_model = True, early_stopping_rounds = 300)\n",
    "model_cat.fit(x_comb_train, y_comb_train, eval_set = (x_comb_val, y_comb_val))\n",
    "y_pred_comb_cat = model_cat.predict(np.concatenate((prob_lgbm, prob_xgb), axis = 1))\n",
    "accuracy_score(y_test, y_pred_comb_cat)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "d925ab58-db3f-48bd-b5f6-1fdd0c661c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(227743, 6)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_new_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "8761168c-6204-4939-8bea-48046c239aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rf = RandomForestClassifier(n_estimators = 100, random_state = 42)\n",
    "model_rf.fit(x_new_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "489ed993-8b5a-4a55-9ff2-84e6939b1a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81.74683673992111"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_rf_combine = model_rf.predict(np.concatenate((prob_lgbm, prob_xgb), axis = 1))\n",
    "accuracy_score(y_test, y_pred_rf_combine)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "c7e829d8-a9de-45f1-85af-890bcbb36548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1, ..., 0, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model to a file\n",
    "with open('model_lgbm.pkl', 'wb') as f:\n",
    "    pickle.dump(model_lgbm, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4d86685-2476-4905-8936-6a76f4ae81a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 81.90973823062343%\n",
      "Balanced Accuracy = 76.85209122219896%\n",
      "Precision = 81.51138567985998%\n",
      "F1 Score = 81.60084976226665%\n"
     ]
    }
   ],
   "source": [
    "# Load the model from the file\n",
    "mode = 'weighted'\n",
    "with open('model_lgbm.pkl', 'rb') as f:\n",
    "    model_lgbm_loaded = pickle.load(f)\n",
    "y_pred_lgbm = model_lgbm_loaded.predict(x_test)\n",
    "print(f'Accuracy = {accuracy_score(y_test, y_pred_lgbm)*100}%')\n",
    "print(f'Balanced Accuracy = {balanced_accuracy_score(y_test, y_pred_lgbm)*100}%')\n",
    "print(f'Precision = {precision_score(y_test, y_pred_lgbm, average = mode)*100}%')\n",
    "print(f'F1 Score = {f1_score(y_test, y_pred_lgbm, average = mode)*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "5f24d2e7-0066-48dc-847d-2999ed7e1cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1, ..., 0, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model to a file\n",
    "with open('model_xgb.pkl', 'wb') as f:\n",
    "    pickle.dump(model_xgb, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48e60089-b007-44dd-b320-4174de3285f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 81.57471441012243%\n",
      "Accuracy = 76.65638719500657%\n",
      "Precision = 81.19682328474047%\n",
      "F1 Score = 81.30774289585752%\n"
     ]
    }
   ],
   "source": [
    "mode = 'weighted'\n",
    "with open('model_xgb.pkl', 'rb') as f:\n",
    "    model_xgb_loaded = pickle.load(f)\n",
    "y_pred_xgb = model_xgb_loaded.predict(x_test)\n",
    "print(f'Accuracy = {accuracy_score(y_test, y_pred_xgb)*100}%')\n",
    "print(f'Balanced Accuracy = {balanced_accuracy_score(y_test, y_pred_xgb)*100}%')\n",
    "print(f'Precision = {precision_score(y_test, y_pred_xgb, average = mode)*100}%')\n",
    "print(f'F1 Score = {f1_score(y_test, y_pred_xgb, average = mode)*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "10bc537a-6074-4ba5-951c-4468b2c93f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in c:\\users\\dell\\anaconda3\\envs\\ddp\\lib\\site-packages (4.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\dell\\anaconda3\\envs\\ddp\\lib\\site-packages (from lightgbm) (1.24.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\dell\\anaconda3\\envs\\ddp\\lib\\site-packages (from lightgbm) (1.11.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e4284a2d-4e74-4b7d-bccd-4b87ad720f29",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_lgbm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model_lgbm\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_lgbm' is not defined"
     ]
    }
   ],
   "source": [
    "del model_lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e68db486-e5a4-4389-8498-fdebc08d9789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010145 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2236\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.911343\n",
      "[LightGBM] [Info] Start training from score -1.830111\n",
      "[LightGBM] [Info] Start training from score -0.826403\n",
      "The accuracy of the LGBM model is 81.88924747707597%.\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "model_lgbm = lgb.LGBMClassifier(\n",
    "    boosting_type = 'gbdt',\n",
    "    objective = 'multiclass',  # Change this based on your specific problem (e.g., 'multiclass' for multi-class classification)\n",
    "    num_leaves = 1000,\n",
    "    learning_rate = 0.1,\n",
    "    n_estimators = 1000\n",
    ")\n",
    "model_lgbm.fit(x_train, y_train)\n",
    "y_pred_lgbm = model_lgbm.predict(x_test)\n",
    "print(f'The accuracy of the LGBM model is {accuracy_score(y_test, y_pred_lgbm)*100}%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "cf91ee64-f252-46a8-937a-61d001fd2d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "ea7b6392-0444-4fa5-a897-b112a27b29c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010788 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2241\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.918318\n",
      "[LightGBM] [Info] Start training from score -1.822684\n",
      "[LightGBM] [Info] Start training from score -0.822758\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003462 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2238\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.915353\n",
      "[LightGBM] [Info] Start training from score -1.829569\n",
      "[LightGBM] [Info] Start training from score -0.822933\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013941 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2239\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.914983\n",
      "[LightGBM] [Info] Start training from score -1.827860\n",
      "[LightGBM] [Info] Start training from score -0.823896\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012054 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2241\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.913298\n",
      "[LightGBM] [Info] Start training from score -1.831794\n",
      "[LightGBM] [Info] Start training from score -0.823996\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011283 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2242\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.909720\n",
      "[LightGBM] [Info] Start training from score -1.825711\n",
      "[LightGBM] [Info] Start training from score -0.829516\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011160 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2239\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.906441\n",
      "[LightGBM] [Info] Start training from score -1.824384\n",
      "[LightGBM] [Info] Start training from score -0.833045\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003771 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2240\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.915929\n",
      "[LightGBM] [Info] Start training from score -1.828953\n",
      "[LightGBM] [Info] Start training from score -0.822633\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014507 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2241\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.907814\n",
      "[LightGBM] [Info] Start training from score -1.829603\n",
      "[LightGBM] [Info] Start training from score -0.829843\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001952 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2240\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.914133\n",
      "[LightGBM] [Info] Start training from score -1.819327\n",
      "[LightGBM] [Info] Start training from score -0.827819\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013403 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2240\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.910962\n",
      "[LightGBM] [Info] Start training from score -1.821631\n",
      "[LightGBM] [Info] Start training from score -0.829881\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010786 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2239\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.910280\n",
      "[LightGBM] [Info] Start training from score -1.822548\n",
      "[LightGBM] [Info] Start training from score -0.830171\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011539 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2240\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.910894\n",
      "[LightGBM] [Info] Start training from score -1.831417\n",
      "[LightGBM] [Info] Start training from score -0.826338\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013103 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2240\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.912451\n",
      "[LightGBM] [Info] Start training from score -1.827143\n",
      "[LightGBM] [Info] Start training from score -0.826476\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011760 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2239\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.910443\n",
      "[LightGBM] [Info] Start training from score -1.834403\n",
      "[LightGBM] [Info] Start training from score -0.825661\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015894 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2238\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.913107\n",
      "[LightGBM] [Info] Start training from score -1.828099\n",
      "[LightGBM] [Info] Start training from score -0.825523\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004871 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2238\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.910361\n",
      "[LightGBM] [Info] Start training from score -1.836604\n",
      "[LightGBM] [Info] Start training from score -0.824935\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018042 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2240\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.912929\n",
      "[LightGBM] [Info] Start training from score -1.825098\n",
      "[LightGBM] [Info] Start training from score -0.826790\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019359 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2241\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.911713\n",
      "[LightGBM] [Info] Start training from score -1.823567\n",
      "[LightGBM] [Info] Start training from score -0.828473\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025218 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2241\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.907515\n",
      "[LightGBM] [Info] Start training from score -1.833578\n",
      "[LightGBM] [Info] Start training from score -0.828661\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012343 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2239\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.913093\n",
      "[LightGBM] [Info] Start training from score -1.833578\n",
      "[LightGBM] [Info] Start training from score -0.823533\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003295 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2240\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.909693\n",
      "[LightGBM] [Info] Start training from score -1.826700\n",
      "[LightGBM] [Info] Start training from score -0.829177\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013270 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2239\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.911358\n",
      "[LightGBM] [Info] Start training from score -1.834196\n",
      "[LightGBM] [Info] Start training from score -0.824897\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015355 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2242\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.909748\n",
      "[LightGBM] [Info] Start training from score -1.832240\n",
      "[LightGBM] [Info] Start training from score -0.827091\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008057 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2242\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.909720\n",
      "[LightGBM] [Info] Start training from score -1.832651\n",
      "[LightGBM] [Info] Start training from score -0.826966\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016279 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2240\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.907365\n",
      "[LightGBM] [Info] Start training from score -1.832995\n",
      "[LightGBM] [Info] Start training from score -0.829013\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005987 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2240\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.908331\n",
      "[LightGBM] [Info] Start training from score -1.833750\n",
      "[LightGBM] [Info] Start training from score -0.827844\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013644 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2240\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.909462\n",
      "[LightGBM] [Info] Start training from score -1.834609\n",
      "[LightGBM] [Info] Start training from score -0.826489\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004069 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2238\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.917150\n",
      "[LightGBM] [Info] Start training from score -1.827826\n",
      "[LightGBM] [Info] Start training from score -0.821934\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015470 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2240\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.913080\n",
      "[LightGBM] [Info] Start training from score -1.828851\n",
      "[LightGBM] [Info] Start training from score -0.825273\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012828 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2241\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.917178\n",
      "[LightGBM] [Info] Start training from score -1.822107\n",
      "[LightGBM] [Info] Start training from score -0.824008\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005070 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2240\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.915311\n",
      "[LightGBM] [Info] Start training from score -1.833956\n",
      "[LightGBM] [Info] Start training from score -0.821372\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003590 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2241\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.913162\n",
      "[LightGBM] [Info] Start training from score -1.817365\n",
      "[LightGBM] [Info] Start training from score -0.829441\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016194 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2239\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.912656\n",
      "[LightGBM] [Info] Start training from score -1.827246\n",
      "[LightGBM] [Info] Start training from score -0.826251\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013403 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2238\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.913832\n",
      "[LightGBM] [Info] Start training from score -1.819598\n",
      "[LightGBM] [Info] Start training from score -0.827995\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020969 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2240\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.912533\n",
      "[LightGBM] [Info] Start training from score -1.820885\n",
      "[LightGBM] [Info] Start training from score -0.828711\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013111 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2237\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.905259\n",
      "[LightGBM] [Info] Start training from score -1.833578\n",
      "[LightGBM] [Info] Start training from score -0.830750\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014709 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2239\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.910007\n",
      "[LightGBM] [Info] Start training from score -1.835125\n",
      "[LightGBM] [Info] Start training from score -0.825799\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003164 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2240\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.912915\n",
      "[LightGBM] [Info] Start training from score -1.833716\n",
      "[LightGBM] [Info] Start training from score -0.823646\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014566 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2242\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.918318\n",
      "[LightGBM] [Info] Start training from score -1.821631\n",
      "[LightGBM] [Info] Start training from score -0.823145\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016981 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2239\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.909066\n",
      "[LightGBM] [Info] Start training from score -1.831966\n",
      "[LightGBM] [Info] Start training from score -0.827819\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016183 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2239\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.910471\n",
      "[LightGBM] [Info] Start training from score -1.833098\n",
      "[LightGBM] [Info] Start training from score -0.826113\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012068 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2240\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.911317\n",
      "[LightGBM] [Info] Start training from score -1.832686\n",
      "[LightGBM] [Info] Start training from score -0.825486\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016092 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2239\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.910430\n",
      "[LightGBM] [Info] Start training from score -1.827621\n",
      "[LightGBM] [Info] Start training from score -0.828158\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016429 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2240\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.903903\n",
      "[LightGBM] [Info] Start training from score -1.834506\n",
      "[LightGBM] [Info] Start training from score -0.831670\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014894 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2239\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.915202\n",
      "[LightGBM] [Info] Start training from score -1.823737\n",
      "[LightGBM] [Info] Start training from score -0.825210\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003575 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2238\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.918167\n",
      "[LightGBM] [Info] Start training from score -1.828816\n",
      "[LightGBM] [Info] Start training from score -0.820648\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017713 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2239\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.913750\n",
      "[LightGBM] [Info] Start training from score -1.825643\n",
      "[LightGBM] [Info] Start training from score -0.825837\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001940 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2242\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.909925\n",
      "[LightGBM] [Info] Start training from score -1.835228\n",
      "[LightGBM] [Info] Start training from score -0.825837\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021872 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2240\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.911549\n",
      "[LightGBM] [Info] Start training from score -1.834953\n",
      "[LightGBM] [Info] Start training from score -0.824446\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014195 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2240\n",
      "[LightGBM] [Info] Number of data points in the train set: 227743, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.905761\n",
      "[LightGBM] [Info] Start training from score -1.827007\n",
      "[LightGBM] [Info] Start training from score -0.832704\n",
      "The accuracy of the bagging LGBM model is 79.76367330908593%.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "base_estimator = lgb.LGBMClassifier(\n",
    "    boosting_type = 'gbdt',\n",
    "    objective = 'multiclass',  # Change this based on your specific problem (e.g., 'multiclass' for multi-class classification)\n",
    "    num_leaves = 50,\n",
    "    learning_rate = 0.05,\n",
    "    n_estimators = 100\n",
    ")\n",
    "model_bag = BaggingClassifier(\n",
    "    base_estimator=base_estimator,\n",
    "    n_estimators=50,       # Number of base estimators to train\n",
    "    max_samples=0.8,       # Fraction of samples to draw from X to train each base estimator\n",
    "    max_features=1.0,      # Fraction of features to draw from X to train each base estimator\n",
    "    bootstrap=True,        # Whether samples are drawn with replacement\n",
    "    bootstrap_features=False,  # Whether features are drawn with replacement\n",
    "    random_state=42\n",
    ")\n",
    "model_bag.fit(x_train, y_train)\n",
    "y_pred_bag = model_bag.predict(x_test)\n",
    "print(f'The accuracy of the bagging LGBM model is {accuracy_score(y_test, y_pred_bag)*100}%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "a37cb09e-2641-4593-87bf-0218040c6ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20136,  1228,  2259],\n",
       "       [ 1824,  5464,  2200],\n",
       "       [ 1957,  1367, 22128]], dtype=int64)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7de99146-12ca-4ddb-9431-73fc7d95b93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cat.save_model('cat1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bf05d8f1-e06b-4adc-a0a1-e4d220e1a6c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x2061e452190>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loaded = CatBoostClassifier()\n",
    "model_loaded.load_model('cat1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "65a8b149-9c97-4cb1-a6ff-da479b955e06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1],\n",
       "       [-1],\n",
       "       [-1],\n",
       "       ...,\n",
       "       [-1],\n",
       "       [ 1],\n",
       "       [ 1]], dtype=int64)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loaded.predict(x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
